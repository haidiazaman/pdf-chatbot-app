{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df77ba3-6ce2-429b-9c7e-d9c94d391b55",
   "metadata": {},
   "source": [
    "quickstart tutorial: https://medium.com/@altudev/run-llms-locally-with-ollama-llama-2-mistral-gemma-more-649c96d4f114\n",
    "ollama-langchain: https://python.langchain.com/v0.2/docs/integrations/chat/ollama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a09be5e-d385-4122-b6d6-628df529f0a0",
   "metadata": {},
   "source": [
    "On Mac, the models will be download to ~/.ollama/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22506011-d1fa-4e0d-9682-55c8e626a573",
   "metadata": {},
   "source": [
    "in terminal\n",
    "\n",
    "ollama list -> view LLM models downloaded to your local\n",
    "ollama run <name-of-model> -> chat directly with your model\n",
    "ollama help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f49b17-e4d6-4fe3-b864-8fbbf1361b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I748920/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = 'llama3.1',\n",
    "    temperature=0 # The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3419a232-4714-4573-b8b4-d36db9e4bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "message = \"explain bernoulli's law\"\n",
    "start_time = time.time()\n",
    "response = llm.invoke(input=message)\n",
    "time_taken = time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0b229a5-f8a2-45ee-9b4d-f0f7ffafcdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A fundamental concept in fluid dynamics!\\n\\nBernoulli's Law, also known as Bernoulli's Principle, is a mathematical relationship that describes the behavior of fluids (liquids and gases) in motion. It was first formulated by Swiss mathematician Daniel Bernoulli in 1738.\\n\\n**The Basic Idea**\\n\\nBernoulli's Law states that an increase in the speed of a fluid occurs simultaneously with a decrease in pressure. Conversely, a decrease in speed is accompanied by an increase in pressure.\\n\\n**Mathematical Formulation**\\n\\nIn mathematical terms, Bernoulli's Law can be expressed as:\\n\\nP + 1/2ρv² + ρgh = constant\\n\\nwhere:\\n\\n* P is the pressure of the fluid\\n* ρ (rho) is the density of the fluid\\n* v is the velocity of the fluid\\n* g is the acceleration due to gravity (approximately 9.81 m/s² on Earth)\\n* h is the height of the fluid above a reference point\\n\\n**Interpretation**\\n\\nThe equation shows that pressure (P), kinetic energy (1/2ρv²), and potential energy (ρgh) are all related. When the velocity (v) of the fluid increases, its kinetic energy also increases, which means that its pressure (P) decreases.\\n\\nConversely, when the velocity decreases, the kinetic energy decreases, and the pressure increases.\\n\\n**Examples**\\n\\n1. **Airplane Wings**: The curved upper surface of an airplane wing deflects air downward, creating a region of lower air pressure above the wing and higher air pressure below it. This difference in pressure generates lift, allowing the plane to fly.\\n2. **Water Flow**: When water flows through a narrow pipe, its velocity increases, and its pressure decreases. Conversely, when water flows through a wider pipe, its velocity decreases, and its pressure increases.\\n3. **Wind**: The wind blowing over a hill or a mountain creates areas of low pressure above the terrain and high pressure below it.\\n\\n**Key Implications**\\n\\nBernoulli's Law has significant implications in various fields:\\n\\n1. **Aerodynamics**: Understanding Bernoulli's Principle is crucial for designing efficient aircraft, wind turbines, and other aerodynamic systems.\\n2. **Hydraulics**: The law helps engineers design pipes, pumps, and other hydraulic systems that minimize energy losses and maximize efficiency.\\n3. **Weather Forecasting**: Bernoulli's Law plays a role in understanding weather patterns, such as the formation of high and low-pressure systems.\\n\\nIn summary, Bernoulli's Law is a fundamental principle that describes the relationship between fluid velocity, pressure, and potential energy. Its applications are diverse and far-reaching, influencing various fields from aerodynamics to hydraulics and beyond!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb0f5813-74a8-41f1-8270-b3104a8c4d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.089190244674683"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eae44c-5e4c-4a88-9be1-0c58f6711401",
   "metadata": {},
   "source": [
    "time taken is pretty long as the model is on ur cpu ram and not on gpu\n",
    "OpenAI API is like 4s but need to pay\n",
    "\n",
    "can purchase AWS or GCP, you just buy a 8gb EC2 5 bucks a month for GPU access then host your model on the server and call an API like calling OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fac31a-03ec-454d-90a5-c52fa15a4268",
   "metadata": {},
   "source": [
    "No matter how inefficient your code is, whether O(n^2) or (n!), the LLM is always the bottleneck in terms of speed and eff when running local\n",
    "If you use openai or Groq, then inefficient coding will be the bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f838447-d75b-4bc5-ac79-e374ac8e2a05",
   "metadata": {},
   "source": [
    "to reduce compute time, can also use smaller model - TinyLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c3187-5d35-4471-946f-b11741851a07",
   "metadata": {},
   "source": [
    "ollama run tinyllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e0f3224-71be-47ea-83f1-3e5f81dbe9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model='tinyllama',\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff59ed2-e934-48f0-8b08-5eb10d8ccda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "message = \"explain bernoulli's law\"\n",
    "start_time = time.time()\n",
    "response = llm.invoke(input=message)\n",
    "time_taken = time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "582e8c6a-f8ff-4f46-a75e-99700282a91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bernoulli's Law is a mathematical formula that describes the probability of success or failure in a random experiment. It states that for any given outcome, the probability of success (or the probability of getting a positive result) increases with the number of attempts made until a certain threshold is reached. This threshold is called the Bernoulli limit and it depends on the type of experiment being conducted.\\n\\nIn a Bernoulli trial, there are two possible outcomes: either the experiment produces a positive result (success), or it does not (failure). The probability of success increases with the number of attempts made until a certain threshold is reached. For example, if the threshold is set at 2 attempts and the probability of success is 0.5 (half chance), then for every 3 attempts made, the probability of success will increase to 1.0 (full chance).\\n\\nBernoulli's Law can be used to calculate the probability of success or failure in a random experiment. For example, if we conduct a Bernoulli trial with a threshold of 2 attempts and the probability of success is 0.5, then the probability of getting a positive result (success) is:\\n\\nP(Success) = P(1 + 1) / 2 = 0.5 * 0.5 / 2 = 0.25\\n\\nThis means that for every 3 attempts made, we have a 25% chance of getting a positive result (success).\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4789e590-9e07-4fac-a2f5-ee32b17ed30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.524280071258545"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811dee81-c97a-49f7-a03e-e77d92dc8f0e",
   "metadata": {},
   "source": [
    "note\n",
    "- when using jupyter notebook, the LLM inference when using ollama is really slow compared to using terminal which instanteneous even for llama3.1\n",
    "- can craete the app in notebook but call the model in terminal\n",
    "- need to check the context length that the model you used was trained on for better results\n",
    "    - https://huggingface.co/blog/llama31#:~:text=Why%20is%20Llama%203.1%20so,Multilingual%20capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515a419-20d0-4df4-b48c-2e030b1cbd65",
   "metadata": {},
   "source": [
    "e.g. What does the 128k context window mean for ChatGPT Plus users?\n",
    "\n",
    "So there is a limit to how many words and symbols an AI model can keep track of. The current version of ChatGPT can keeps track of 8k tokens or about 5-6 thousand words. My math may be off on that but it’s not far off. After you reach that limit the model will forget what was going on at the start of the conversation. Larger window means it will keep track of more info. It’s probably the best part of the update since it is a 16x increase of context window/memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
