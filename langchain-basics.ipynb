{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e9420e-0969-427b-a271-8118f3f2f270",
   "metadata": {},
   "source": [
    "Joule Conversational Search\n",
    "to learn \n",
    "https://python.langchain.com/v0.2/docs/tutorials/\n",
    "-Basics \n",
    "<!-- Build a Simple LLM Application with LCEL -->\n",
    "<!-- Build a Chatbot -->\n",
    "<!-- Build vector stores and retrievers -->\n",
    "Build an Agent\n",
    "\n",
    "-Working with external knowledge\n",
    "Build a Retrieval Augmented Generation (RAG) Application\n",
    "Build a Conversational RAG Application\n",
    "Build a PDF ingestion and Question/Answering system\n",
    "\n",
    "\n",
    "Automating manual testing process\n",
    "-how to use free model on langchain\n",
    "-ragas: how to use the free model instead of openai by default\n",
    "-relating metrics to those in excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e98d06-c044-4263-821c-059ab293961c",
   "metadata": {},
   "source": [
    "# Build a Simple LLM Application with LCEL\n",
    "\n",
    "In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3479e38-130b-483f-8382-adfcd883234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass \n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model='gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d04bf95-3bbe-4b31-8c16-f8715c828903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x16858e040>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1685929d0>, root_client=<openai.OpenAI object at 0x1059ad6a0>, root_async_client=<openai.AsyncOpenAI object at 0x16858e070>, model_name='gpt-4', openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23bb9deb-3501-43ca-aac2-7efff04c3edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='salut!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-49928e79-6aea-45f5-9716-0dcaff45da2d-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='Translate the following from English to French'),\n",
    "    HumanMessage(content='hi!')\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea44ae0-ce19-424a-96fe-fe44bb22874f",
   "metadata": {},
   "source": [
    "Notice that the response from the model is an AIMessage. This contains a string response along with other metadata about the response. Oftentimes we may just want to work with the string response. We can parse out just this response by using a simple output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "754f6e70-7046-4a0b-874d-c2737710e3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salut!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser() # use this to parse only the response content without the other details\n",
    "result = model.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68718c-c61e-4ce2-8ff4-8de722811b8c",
   "metadata": {},
   "source": [
    "instead of doing it sequentially, can use a chain instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c205ff8c-7707-489b-9dc8-97bda4c9bc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salut!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = model | parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077dffc-595a-40bd-9093-27ce58a322bf",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a5243-1905-4458-822e-87025dd57ae0",
   "metadata": {},
   "source": [
    "usually prompts are user inputs, but to make it a comprehensive prompt where a user input is just 1 part of the prompt, should use a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "caf5ed67-fd52-479c-99be-c6f6879c4317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following into malay'), HumanMessage(content='where are we eating dinner tonight?')])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# has 2 parts - system and user\n",
    "# - this will become SystemMessage and HumanMessage\n",
    "\n",
    "system_template = \"Translate the following into {language}\"\n",
    "text_template = \"{text}\"\n",
    "# the str in {} will become the dict keys for user inputs during invoke\n",
    "\n",
    "# create prompt template, combination of system template and simpler template for user input template\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',system_template),\n",
    "        ('user',text_template)\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = prompt_template.invoke(\n",
    "    {\n",
    "        'language':'malay',\n",
    "        'text':\"where are we eating dinner tonight?\"\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52d2b666-7eaa-4862-ba30-e2574ade5001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into malay'),\n",
       " HumanMessage(content='where are we eating dinner tonight?')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to access the messages directly\n",
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af3028-8c14-4794-8ba4-21582d14200a",
   "metadata": {},
   "source": [
    "chain together components with LCEL (Langchain Expression Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea4e2cad-9152-49f5-9f92-82761550ccdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mari pergi ke zoo di mana terdapat banyak gajah bermain dengan harimau.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | model | parser\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        'language': 'Malay',\n",
    "        'text': 'Lets go to the zoo where there are many elephants playing with tigers'\n",
    "    }\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8970e-7a50-40ed-b591-575d523496da",
   "metadata": {},
   "source": [
    "using chain.invoke will directly give the output message, instead of model.invoke which will return an AIMessage which you then need to pass thru StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee92eee-e65c-4c61-9abc-d4993954ee20",
   "metadata": {},
   "source": [
    "# Summary of Chat usage\n",
    "\n",
    "1. Create PromptTemplate\n",
    "2. Initialise model\n",
    "3. Create StrOutputParser\n",
    "4. Create chain\n",
    "5. chain.invoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b8097-bed0-4dfe-9a71-ea109c44fd5b",
   "metadata": {},
   "source": [
    "Notes on SystemMessage vs HumanMessage vs AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54119969-1626-4673-90cc-9d0ea974eaf3",
   "metadata": {},
   "source": [
    "SystemMessage\n",
    "\n",
    "Message for priming AI behavior.\n",
    "The system message is usually passed in as the first of a sequence of input messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45127e-8fa9-4808-8a0f-3f6d7831747d",
   "metadata": {},
   "source": [
    "HumanMessages\n",
    "\n",
    "Message from a human.\n",
    "HumanMessages are messages that are passed in from a human to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae33709-7bca-4df9-8d59-60e234f01505",
   "metadata": {},
   "source": [
    "AIMessage\n",
    "\n",
    "Message from an AI.\n",
    "AIMessage is returned from a chat model as a response to a prompt.\n",
    "This message represents the output of the model and consists of both the raw output as returned by the model together standardized fields (e.g., tool calls, usage metadata) added by the LangChain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741f549-c5f5-46a9-bf17-d40e33a6a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. SystemMessage should be used to \"instruct/inform\" the chat model about its task, characteristics etc\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant! Your name is Bob.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is your name?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define a chat model and invoke it with the messages\n",
    "print(model.invoke(messages))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
