{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb039ef-edb0-48d1-8892-96f3b565a122",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# python script v2 issues -> fix in v3\n",
    "\n",
    "* discrepancy in the results of retriever = vectorstore.as_retriever(search_type='similarity',search_kwargs = {'k':5}) vs get_retrieved_documents_with_scores manual function\n",
    "    * the retriever results for the former is not returning the top K in correct order but manual function is\n",
    "    * means the history_aware_retriever is also not getting the top K results\n",
    "* if change retriever to manual method, means history_aware_retriever wont work already, so need to manually do the refine_query process of the history_aware_retriever\n",
    "    * history_aware_retriever has 2 parts: 1. refining query with convo context + 2. retrieval with new refined_query\n",
    "    * try do part 1 with the ollama package without using langchain ollama, and then dont need to stream the output for this part\n",
    "    * double check that the output of the refined_query is correct\n",
    "    * then integrate with the the fixed manual retrieval process\n",
    "* add functionality of setting sim_score_threshold as part of python script argparse\n",
    "    * references must be top K > sim_score only\n",
    "    * if no documents >sim_score then return empty list, then in prompt or ?? if empty list then the LLM must say it doesnt know in the answer -> need to think where best to implemetn this part\n",
    " \n",
    "* SUMMARY of v3 fixes\n",
    "    * fix retrieval process of using similarity score -> currently not returning top K correctly, so use manual function\n",
    "        * but still using langchain embeddings, vectorstore and retrieval \n",
    "    * get rid of using history_aware retriever, need to create a pipeline for query refining with convo history\n",
    "        * then integrate with the fixed retriever\n",
    "    * add functionality of setting sim_score_threshold as part of python script argparse\n",
    "        * need to deal with the empty list returned, LLM say it doesnt know\n",
    "- v3 fixes will improve retrieval process and reduce hallucinations with the last added functionality\n",
    "- v4 will be to remove langchain usage of embeddings, vectorstore and retrieval (refer to Nic script and class in telegram chat)\n",
    "- v5 will be remove langchain usage of ChatMessageHistory, HumanMessage, AIMessage (maybe RecursiveCharacterTextSplitter and chunking can just us\n",
    "- e langchain, not big issue, trivial easy fix if required in the future)\n",
    "\n",
    "** consideration: over time the conversation size grows and will need to be aware of total number of tokens - need to Manage Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ef248a9-6152-4a91-be91-3cf628e25924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "# import os\n",
    "import glob\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "# from langchain_core.chat_history import BaseChatMessageHistory\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import warnings\n",
    "# from urllib3.exceptions import NotOpenSSLWarning\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Suppress NotOpenSSLWarning from urllib3\n",
    "warnings.filterwarnings(\"ignore\", module='urllib3')\n",
    "\n",
    "\n",
    "def load_pdfs(file_paths):\n",
    "    \"\"\"\n",
    "    file_paths must end with .pdf\n",
    "    PyPDFLoader auto splits the pdf into pages, each page is 1 Document object split by page number\n",
    "    note that the splitting by page number is not perfect, the actual page number might be +/- 1-2pages.\n",
    "\n",
    "    returns a dict of key: file_path and value: list of document objects\n",
    "    \"\"\"\n",
    "    documents_dict = {}   \n",
    "    for f in tqdm(file_paths):\n",
    "        loader = PyPDFLoader(file_path = f)\n",
    "        documents = loader.load()\n",
    "        documents_dict[f] = documents\n",
    "    return documents_dict\n",
    "\n",
    "def chunk_list_of_documents(documents):\n",
    "    \"\"\"\n",
    "    input a list of documents as Document objects\n",
    "\n",
    "    output a list of chunks as Document objects\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 100, # using 20% is a good start\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)    \n",
    "    return chunks\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    \"\"\"\n",
    "    if session_id exists, function returns the ChatMessageHistory of that session_id.\n",
    "\n",
    "    if session_id does not exists, function instantiates a new ChatMessageHistory.\n",
    "    \"\"\"\n",
    "    if session_id not in chat_history_store:\n",
    "        chat_history_store[session_id] = ChatMessageHistory()\n",
    "    return chat_history_store[session_id]\n",
    "\n",
    "def create_huggingface_retriever(folder_path,embedding_model_name):\n",
    "    \"\"\"\n",
    "    folder_path is type str, absolute folder path of the pdf files' location.\n",
    "    embedding_model_name type str, take key from HG website.\n",
    "\n",
    "    1. uses load_pdfs and chunk_list_of_documents functions to\n",
    "    get chunks across the different input pdfs.\n",
    "    2. sets up huggingface embedding model based on embedding_model_name passed\n",
    "    3. sets up the vector db and adds the embeds the chunks into the vector db.\n",
    "    4. sets up retriever object from the filled vector db\n",
    "\n",
    "    output: retriever_hf created from the HugginFaceEmbeddings\n",
    "    \"\"\"\n",
    "    files_paths = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    # load documents from file paths\n",
    "    print(\"loading pdfs...\")\n",
    "    documents_dict = load_pdfs(file_paths=files_paths)\n",
    "\n",
    "    # chunk documents\n",
    "    print()\n",
    "    print(\"chunking documents...\")\n",
    "    all_chunks = []\n",
    "    for key in tqdm(documents_dict.keys()):\n",
    "        documents = documents_dict[key]\n",
    "        chunks = chunk_list_of_documents(documents=documents)\n",
    "        all_chunks.extend(chunks)\n",
    "    print(f\"number of chunks: {len(all_chunks)}\")\n",
    "\n",
    "    # setup embedding model\n",
    "    print()\n",
    "    print(\"instantiating HuggingFaceEmbeddings...\")\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    "    hf_embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    print(\"hf_embedding_model created!\")\n",
    "\n",
    "    # setup vectordb, using HF embedding model\n",
    "    start_time=time.time()\n",
    "    print()\n",
    "    print(\"start process of embedding chunks into vector database...\")\n",
    "    vectorstore_hf = InMemoryVectorStore.from_documents(\n",
    "        documents=all_chunks,\n",
    "        embedding=hf_embedding_model\n",
    "    )\n",
    "    print(\"all chunks embedded into vector database!\",f\"time taken: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "    # setup retrieval and test with a query and gt_context\n",
    "    retriever_hf = vectorstore_hf.as_retriever(\n",
    "        search_type='similarity',\n",
    "        search_kwargs = {'k':5}\n",
    "    )\n",
    "    print(\"retriever created!\")\n",
    "\n",
    "    return vectorstore_hf,retriever_hf\n",
    "\n",
    "def instantiate_history_aware_retriever(retriever_hf,llm_model_name):\n",
    "    \"\"\"\n",
    "    retriever_hf is the output of the create_huggingface_retriever function.\n",
    "    llm_model_name is type str, can be taken from langchain website.\n",
    "\n",
    "    use langchain to create a history_aware_retriever. \n",
    "    The LLM used here is from langchain, not Ollama.\n",
    "\n",
    "    output: langchain history_aware_retriever object.\n",
    "    \"\"\"\n",
    "\n",
    "    # setup llm chat model using ollama\n",
    "    llm_model = ChatOllama(\n",
    "        model=llm_model_name,\n",
    "        temperature=0 # increase temp for more creative answers\n",
    "    ) \n",
    "\n",
    "    # setup system contextualise input prompt\n",
    "    system_contextualise_input_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "    system_input_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_contextualise_input_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # instantiate the history-aware retriever:\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm=llm_model,\n",
    "        retriever=retriever_hf,\n",
    "        prompt=system_input_prompt\n",
    "    )\n",
    "\n",
    "    return history_aware_retriever\n",
    "\n",
    "def format_chat_history(chat_history):\n",
    "    \"\"\"\n",
    "    input: chat_history is a langchain_community ChatMessageHistory object.\n",
    "\n",
    "    function formats the messages into dict format \n",
    "    instead of Langchain AI and HumanMessage objects.\n",
    "    this function returns empty list if there is no chat_history\n",
    "    \"\"\"\n",
    "\n",
    "    formatted_chat_history = [\n",
    "        {\"role\": \"human\", \"content\": message.content} if isinstance(message, HumanMessage) else\n",
    "        {\"role\": \"ai\", \"content\": message.content}\n",
    "        for message in chat_history.messages\n",
    "    ]\n",
    "\n",
    "    return formatted_chat_history\n",
    "\n",
    "def manual_rag_with_ollama(retrieved_documents, formatted_chat_history, input_query, ollama_model_name=\"llama3.1\"):\n",
    "    \"\"\"\n",
    "    Manually performs RAG using retrieved documents from history-aware-retriever and streams results from the Ollama model.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_documents (list of Document objects):  output of history-aware-retriever.invoke().\n",
    "        formatted_chat_history (list of dict): output of format_chat_history function.\n",
    "        input_query (str): The user's input query.\n",
    "        ollama_model_name (str): The name of the Ollama model to use.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Format the retrieved documents as context\n",
    "    retrieved_references = \"\\n\\n\".join([doc.page_content for doc in retrieved_documents])\n",
    "    \n",
    "    # Step 2: Create a prompt that integrates the retrieved context and input query\n",
    "    input_prompt = (\n",
    "        f\"You are an assistant for question-answering tasks. You must reference information from the retrieved_references to answer the input_query. \"\n",
    "        f\"You must also reference the formatted_chat_history to take into account conversation flow and to ensure that the response is relevant to both the current query and prior conversation. \"\n",
    "        f\"Use five sentences maximum and keep the answer concise. Also, if the input_query is specifically a yes or no question, you must only answer yes or no.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"retrieved_references: \\n{retrieved_references}\"\n",
    "        \"\\n\\n\"\n",
    "        f\"formatted_chat_history: \\n{formatted_chat_history}\"\n",
    "        \"\\n\\n\"\n",
    "        f\"input_query: \\n{input_query}\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Pass the prompt to the Ollama LLM and stream the response\n",
    "    # print(\"Streaming response from Ollama...\")\n",
    "    print(\"LLM Response:\")\n",
    "\n",
    "    stream = ollama.chat(\n",
    "        model=ollama_model_name,\n",
    "        messages=[{'role': 'user', 'content': input_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    response = ''\n",
    "    # Stream and display the output from Ollama as it generates\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "        response += chunk['message']['content']  # Append each chunk to the answer\n",
    "\n",
    "    return response\n",
    "\n",
    "def main(folder_path,embedding_model_name,llm_model_name):\n",
    "    \"\"\"\n",
    "    folder_path (str): absolute folder path of the pdf files' location.\n",
    "    embedding_model_name (str): model key name that can be taken from HG website.\n",
    "    llm_model_name (str): model key name that can be taken from Ollama website.\n",
    "\n",
    "    main function that implements the entire RAG process. \n",
    "    session_id is hard-coded to '1' for now since there is no persistence.\n",
    "    \"\"\"\n",
    "\n",
    "    session_id = \"1\" # hardcode this for temp fix since there is no persistence implemented\n",
    "\n",
    "    # Initialise embedding and llm model_name\n",
    "    if embedding_model_name is None:\n",
    "        embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    if llm_model_name is None:\n",
    "        llm_model_name = 'llama3.1'\n",
    "\n",
    "    # Create the Hugging Face retriever\n",
    "    retriever_hf = create_huggingface_retriever(folder_path=folder_path,embedding_model_name=embedding_model_name)\n",
    "    # Create the history-aware-retriever\n",
    "    history_aware_retriever = instantiate_history_aware_retriever(retriever_hf=retriever_hf,llm_model_name=llm_model_name)\n",
    "\n",
    "    print()\n",
    "    print(\"########################################\")\n",
    "    print(\"########## START CONVERSATION ##########\")\n",
    "    print(\"########################################\")\n",
    "    print()\n",
    "    print(\"You can now start chatting with the LLM.\")\n",
    "    print(\"Add '--show references' to the end of the input to view the documents referenced by the LLM.\")\n",
    "    print(\"Type 'exit' to stop the conversation.\")\n",
    "    print()\n",
    "\n",
    "    # Initialize the chat loop\n",
    "    while True:\n",
    "        # Get user input\n",
    "        full_user_input = input(\"User input: \")\n",
    "        # Only take the first part of the user input if --show references is used\n",
    "        user_input = full_user_input.split(\"--\")[0]\n",
    "\n",
    "        # End session if user types 'exit'\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Ending session. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # get current chat history\n",
    "        current_chat_history = get_session_history(session_id)\n",
    "        # format current chat history\n",
    "        formatted_chat_history = format_chat_history(current_chat_history)\n",
    "\n",
    "        # retrieve documents using history_aware_retriever\n",
    "        retrieved_documents = history_aware_retriever.invoke(\n",
    "            {\n",
    "                'chat_history':formatted_chat_history,\n",
    "                'input':user_input\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # invoke manual_rag_with_ollama function  and show the results, need to store for chat history update\n",
    "        response = manual_rag_with_ollama(\n",
    "            retrieved_documents=retrieved_documents, \n",
    "            formatted_chat_history=formatted_chat_history, \n",
    "            input_query=user_input, \n",
    "            ollama_model_name=llm_model_name\n",
    "        )\n",
    "\n",
    "        # update chat history with latest user input and LLM output - add the input query and response to the current_chat_history\n",
    "        current_chat_history.add_user_message(user_input)\n",
    "        current_chat_history.add_ai_message(response)\n",
    "        \n",
    "        # Show the references if user requests\n",
    "        print(\"\\n\")\n",
    "        if len(full_user_input.split(\"--\"))>1: # just check for non empty string will suffice jic of misspelling\n",
    "            print(\"### preparing references... ###\")\n",
    "            time.sleep(1)\n",
    "            print(\"References:\")\n",
    "            for i,d in enumerate(retrieved_documents):\n",
    "                time.sleep(1)\n",
    "                print(f\"{i+1} From: page {d.metadata['page']} of {d.metadata['source'].split('/')[-1]}\")\n",
    "                print(f\"Content: {d.page_content}\")\n",
    "                print()\n",
    "\n",
    "\n",
    "# if __name__==\"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"Directly use Conversational RAG with custom PDF documents in terminal.\")\n",
    "#     # Add arguments\n",
    "#     parser.add_argument(\"--folder_path\", type=str, help=\"input absolute folder path to folder of pdfs\", required=True)\n",
    "#     parser.add_argument(\"--embedding_model_name\", type=str, help=\"pass the huggingface embedding model name of your choice\", required=False)\n",
    "#     parser.add_argument(\"--llm_model_name\", type=str, help=\"pass the ollama llm model name of your choice\", required=False)\n",
    "#     # note that the llm_model_name here is used as the model key for both the langchain ChatOllama model for the history_aware_retriever \n",
    "#     # and python Ollama model for the LLM answer generation, if a diff key is used and both packages have different names, there will be issues\n",
    "\n",
    "#     # Parse the arguments\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # create global variable for chat_history_store\n",
    "#     chat_history_store = {}\n",
    "\n",
    "#     # Run the main chat function \n",
    "#     main(\n",
    "#         folder_path=args.folder_path,\n",
    "#         embedding_model_name=args.embedding_model_name,\n",
    "#         llm_model_name=args.llm_model_name \n",
    "#     )\n",
    "\n",
    "# Issues\n",
    "# current bottleneck is the history_aware_retriever generating contexts with conversation history reference, but 2-3s shud be fine just like chatgpt\n",
    "# 3. if cannot find reelvant references, dont return any \n",
    "    # - context refernces must be > sim_score, cannot just take top K\n",
    "# sample queries\n",
    "# do you know about high dimensional problems in statistical learning? yes or no.\n",
    "# explain in which cases can ridge regression do it with regards to p and N in high dimensional?  --show references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66feeb-9897-42a5-acdd-ff501f507fa3",
   "metadata": {},
   "source": [
    "fix issue of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e754c-1551-4346-8e66-7ffb0f4ab023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9131f635-1804-4c67-8f17-0db293370cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "loading pdfs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "chunking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3498.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks: 18\n",
      "\n",
      "instantiating HuggingFaceEmbeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_embedding_model created!\n",
      "\n",
      "start process of embedding chunks into vector database...\n",
      "all chunks embedded into vector database! time taken: 0.67s\n",
      "retriever created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "session_id = \"1\" # hardcode this for temp fix since there is no persistence implemented\n",
    "\n",
    "# Initialise embedding and llm model_name\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "llm_model_name = 'llama3.1'\n",
    "folder_path = \"/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book\"\n",
    "\n",
    "# Create the Hugging Face retriever\n",
    "vectorstore_hf,retriever_hf = create_huggingface_retriever(folder_path=folder_path,embedding_model_name=embedding_model_name)\n",
    "# Create the history-aware-retriever\n",
    "history_aware_retriever = instantiate_history_aware_retriever(retriever_hf=retriever_hf,llm_model_name=llm_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a0da055-a590-47d6-8667-b1dbe98c0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_store = {}\n",
    "# get current chat history\n",
    "current_chat_history = get_session_history(session_id)\n",
    "# format current chat history\n",
    "formatted_chat_history = format_chat_history(current_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a18c67f5-29cd-494f-999e-d8b336e40074",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"do you know about high dimensional problems in statistical learning? yes or no.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7eebdfff-c208-402f-a803-4eaf0412f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = history_aware_retriever.invoke(\n",
    "    {\n",
    "        'chat_history':formatted_chat_history,\n",
    "        'input':user_input\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72bb2c0a-8cf5-416f-96b7-4a0dde3e7c3a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='4e93f4fe-2de8-4b66-9c82-f39943c40a36', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 800}, page_content='to eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-'),\n",
       " Document(id='c9d819ce-160e-4d48-a2da-ef0d34943e80', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 0, 'start_index': 0}, page_content='This is page 649\\nPrinter: Opaque this\\n18\\nHigh-Dimensional Problems: p≫N\\n18.1 When pis Much Bigger than N\\nIn this chapter we discuss prediction problems in which the n umber of\\nfeaturespis much larger than the number of observations N, often written\\np≫N. Such problems have become of increasing importance, espec ially in\\ngenomics and other areas of computational biology. We will s ee that high\\nvariance and overﬁtting are a major concern in this setting. As a result,'),\n",
       " Document(id='f0650ea8-21e7-446f-b93f-c188745fe827', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 0}, page_content='650 18. High-Dimensional Problems: p≫N1.0 1.5 2.0 2.5 3.0Relative error\\n20 9 220 featuresTest Error\\n1.0 1.5 2.0 2.5 3.0\\n99 35 7100 features\\n1.0 1.5 2.0 2.5 3.0\\n99 87 431000 features\\nEffective Degrees of Freedom\\nFIGURE 18.1. Test-error results for simulation experiments. Shown are box -\\nplots of the relative test errors over 100simulations, for three diﬀerent values\\nofp, the number of features. The relative error is the test error d ivided by the'),\n",
       " Document(id='3a7d6400-2ad3-4619-ba5b-8a407479b465', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 0, 'start_index': 395}, page_content='variance and overﬁtting are a major concern in this setting. As a result,\\nsimple, highly regularized approaches often become the met hods of choice.\\nThe ﬁrst part of the chapter focuses on prediction in both the classiﬁcation\\nand regression settings, while the second part discusses th e more basic\\nproblem of feature selection and assessment.\\nTo get us started, Figure 18.1 summarizes a small simulation study that\\ndemonstrates the “less ﬁtting is better” principle that app lies whenp≫N.'),\n",
       " Document(id='6c6cb75c-533b-4738-8852-fe119113d5bb', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 0, 'start_index': 811}, page_content='demonstrates the “less ﬁtting is better” principle that app lies whenp≫N.\\nFor each of N= 100 samples, we generated pstandard Gaussian features\\nXwith pairwise correlation 0 .2. The outcome Ywas generated according\\nto a linear model\\nY=p∑\\nj=1Xjβj+σε (18.1)\\nwhereεwas generated from a standard Gaussian distribution. For ea ch\\ndataset, the set of coeﬃcients βjwere also generated from a standard Gaus-\\nsian distribution. We investigated three cases: p= 20,100,and 1000. The')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fbb0f-02db-405b-8fb3-1536dacd3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "def retriever(query: str) -> List[Document]:\n",
    "    docs, scores = zip(*vectorstore.similarity_search_with_score(query))\n",
    "    for doc, score in zip(docs, scores):\n",
    "        doc.metadata[\"score\"] = score\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07623cef-0625-4eae-ae38-5596d02cb8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def get_retrieved_documents_with_scores(vectorstore,query,k) -> List[Document]:\n",
    "    docs, scores = zip(*vectorstore.similarity_search_with_score(query=query,k=k))\n",
    "    for doc, score in zip(docs, scores):\n",
    "        doc.metadata[\"score\"] = score\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6638900a-b590-4ffb-9f19-8ac6fb0024c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_retrieved_documents_with_scores(vectorstore=vectorstore_hf,query=user_input,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "074cc2f1-ff27-4b85-94d0-6e7105a52176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6213172417016296\n",
      "page_content='to eﬃciently estimate the high-dimensional covariance mat rix. In that case,\n",
      "more regularization leads to superior prediction performa nce.\n",
      "Thus it is not surprising that the analysis of high-dimensio nal data re-\n",
      "quires either modiﬁcation of procedures designed for the N >pscenario, or\n",
      "entirely new procedures. In this chapter we discuss example s of both kinds\n",
      "ofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-' metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 800, 'score': 0.6213172417016296}\n",
      "\n",
      "0.5919392750876704\n",
      "page_content='This is page 649\n",
      "Printer: Opaque this\n",
      "18\n",
      "High-Dimensional Problems: p≫N\n",
      "18.1 When pis Much Bigger than N\n",
      "In this chapter we discuss prediction problems in which the n umber of\n",
      "featurespis much larger than the number of observations N, often written\n",
      "p≫N. Such problems have become of increasing importance, espec ially in\n",
      "genomics and other areas of computational biology. We will s ee that high\n",
      "variance and overﬁtting are a major concern in this setting. As a result,' metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 0, 'start_index': 0, 'score': 0.5919392750876704}\n",
      "\n",
      "0.5506129053201491\n",
      "page_content='650 18. High-Dimensional Problems: p≫N1.0 1.5 2.0 2.5 3.0Relative error\n",
      "20 9 220 featuresTest Error\n",
      "1.0 1.5 2.0 2.5 3.0\n",
      "99 35 7100 features\n",
      "1.0 1.5 2.0 2.5 3.0\n",
      "99 87 431000 features\n",
      "Effective Degrees of Freedom\n",
      "FIGURE 18.1. Test-error results for simulation experiments. Shown are box -\n",
      "plots of the relative test errors over 100simulations, for three diﬀerent values\n",
      "ofp, the number of features. The relative error is the test error d ivided by the' metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 0, 'score': 0.5506129053201491}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata['score'])\n",
    "    print(d)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b89dbdb3-76b2-43c1-bc71-116ec5c88b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input1 = \"explain in which cases can ridge regression do it with regards to p and N in high dimensional?  --show references\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "abb33da3-e889-43ab-8751-81face9fd3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_retrieved_documents_with_scores(vectorstore=vectorstore_hf,query=user_input1,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f2acc7d-0709-4515-a828-db2532436788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726056829878058\n",
      "page_content='over the 100 simulation runs. The p= 1000 case is designed to mimic the\n",
      "kind of data that we might see in a high-dimensional genomic o r proteomic\n",
      "dataset, for example.\n",
      "We ﬁt a ridge regression to the data, with three diﬀerent valu es for the\n",
      "regularization parameter λ: 0.001, 100, and 1000. When λ= 0.001, this\n",
      "is nearly the same as least squares regression, with a little regularization\n",
      "just to ensure that the problem is non-singular when p > N. Figure 18.1' metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 760, 'score': 0.726056829878058}\n",
      "\n",
      "0.6877578348106839\n",
      "page_content='just to ensure that the problem is non-singular when p > N. Figure 18.1\n",
      "shows boxplots oftherelative testerrorachieved bythediﬀ erentestimators\n",
      "in each scenario. The corresponding average degrees of free dom used in\n",
      "each ridge-regression ﬁt is indicated (computed using form ula (3.50) on\n",
      "page 682). The degrees of freedom is a more interpretable parameter t han\n",
      "λ. We see that ridge regression with λ= 0.001 (20 df) wins when p= 20;\n",
      "λ= 100 (35 df) wins when p= 100, and λ= 1000 (43 df) wins when' metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 1150, 'score': 0.6877578348106839}\n",
      "\n",
      "0.5927353881959819\n",
      "page_content='using the optimal ridge parameter in each of the three cases, the median\n",
      "value of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\n",
      "exceeding 2 was equal to 9.8, 1.2 and 0.0.\n",
      "Ridge regression with λ= 0.001 successfully exploits the correlation in\n",
      "the features when p<N, but cannot do so when p≫N. In the latter case\n",
      "there is not enough information in the relatively small numb er of samples\n",
      "to eﬃciently estimate the high-dimensional covariance mat rix. In that case,' metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 397, 'score': 0.5927353881959819}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata['score'])\n",
    "    print(d)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2e54a-c5c0-407f-8881-3998117571ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def get_retrieved_documents_with_scores(vectorstore,query,k) -> List[Document]:\n",
    "    \"\"\"\n",
    "    vectorstore: vectorstore object\n",
    "    query (str): user input query\n",
    "    k (int): number of top K documents to return\n",
    "    \n",
    "    manual function to get retrieved documents from vectorstore and add score to metadata.\n",
    "    output: List of Document objects.\n",
    "    \"\"\"\n",
    "    docs, scores = zip(*vectorstore.similarity_search_with_score(query=query,k=k))\n",
    "    for doc, score in zip(docs, scores):\n",
    "        doc.metadata[\"score\"] = score\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2c86e-2d10-4a06-99ab-c3153bb66f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933a06a-8553-40c2-a9fd-46c1f8ba7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_hf.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2dc531ab-8a66-4868-9fe3-a546aa5c2743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_core.vectorstores.in_memory.InMemoryVectorStore at 0x347d51f40>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4cfe7a92-ade0-4045-bdb6-75dc14ab8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_hf = vectorstore_hf.as_retriever(\n",
    "    search_type='similarity_score_threshold',\n",
    "    search_kwargs = {\n",
    "        'score_threshold':0.9\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "73b98d21-c347-4777-87fa-00297bc6803b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretriever_hf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_input1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/retrievers.py:252\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    251\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    255\u001b[0m         result,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/retrievers.py:245\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 245\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/vectorstores/base.py:1045\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1044\u001b[0m     docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1045\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_relevance_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_kwargs\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m     docs \u001b[38;5;241m=\u001b[39m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_similarities]\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/vectorstores/base.py:552\u001b[0m, in \u001b[0;36mVectorStore.similarity_search_with_relevance_scores\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs and relevance scores in the range [0, 1].\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m0 is dissimilar, 1 is most similar.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    List of Tuples of (doc, similarity_score).\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    550\u001b[0m score_threshold \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 552\u001b[0m docs_and_similarities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_similarity_search_with_relevance_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    556\u001b[0m     similarity \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, similarity \u001b[38;5;129;01min\u001b[39;00m docs_and_similarities\n\u001b[1;32m    558\u001b[0m ):\n\u001b[1;32m    559\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevance scores must be between\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 0 and 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocs_and_similarities\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    562\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    563\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/vectorstores/base.py:499\u001b[0m, in \u001b[0;36mVectorStore._similarity_search_with_relevance_scores\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_similarity_search_with_relevance_scores\u001b[39m(\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    478\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    479\u001b[0m     k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    481\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    482\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    Default similarity search with relevance scores. Modify if necessary\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m    in subclass.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;124;03m        List of Tuples of (doc, similarity_score)\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m     relevance_score_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_relevance_score_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(query, k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(doc, relevance_score_fn(score)) \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/vectorstores/base.py:440\u001b[0m, in \u001b[0;36mVectorStore._select_relevance_score_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select_relevance_score_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[[\u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    430\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    The 'correct' relevance function\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    may differ depending on a few things, including:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    Vectorstores should define their own selection-based method of relevance.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retriever_hf.invoke(input=user_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a9040-356e-47a8-9a9b-b9822f5ac423",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_hf = InMemoryVectorStore.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=hf_embedding_model\n",
    ")\n",
    "print(\"all chunks embedded into vector database!\",f\"time taken: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "# setup retrieval and test with a query and gt_context\n",
    "retriever_hf = vectorstore_hf.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs = {'k':5}\n",
    ")\n",
    "print(\"retriever created!\")\n",
    "\n",
    "return vectorstore_hf,retriever_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fe1132fd-4c0a-4438-ae63-8a69a6a9b35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2a7fa81b-0bb3-4d4c-86be-18c1bbf57590', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 760, 'score': 0.726056829878058}, page_content='over the 100 simulation runs. The p= 1000 case is designed to mimic the\\nkind of data that we might see in a high-dimensional genomic o r proteomic\\ndataset, for example.\\nWe ﬁt a ridge regression to the data, with three diﬀerent valu es for the\\nregularization parameter λ: 0.001, 100, and 1000. When λ= 0.001, this\\nis nearly the same as least squares regression, with a little regularization\\njust to ensure that the problem is non-singular when p > N. Figure 18.1'),\n",
       " Document(id='ecd4bf7b-fd87-491b-8fb7-4ab23676d431', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 1150, 'score': 0.6877578348106839}, page_content='just to ensure that the problem is non-singular when p > N. Figure 18.1\\nshows boxplots oftherelative testerrorachieved bythediﬀ erentestimators\\nin each scenario. The corresponding average degrees of free dom used in\\neach ridge-regression ﬁt is indicated (computed using form ula (3.50) on\\npage 682). The degrees of freedom is a more interpretable parameter t han\\nλ. We see that ridge regression with λ= 0.001 (20 df) wins when p= 20;\\nλ= 100 (35 df) wins when p= 100, and λ= 1000 (43 df) wins when'),\n",
       " Document(id='0c0b3e98-2590-496a-9c54-02074cec6d03', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 397, 'score': 0.5927353881959819}, page_content='using the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,'),\n",
       " Document(id='5dce10df-48ea-4ffd-805d-027bb5e4c7b1', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 368, 'score': 0.5909602438724204}, page_content='ofp, the number of features. The relative error is the test error d ivided by the\\nBayes error, σ2. From left to right, results are shown for ridge regression wit h\\nthree diﬀerent values of the regularization parameter λ:0.001,100and1000. The\\n(average) eﬀective degrees of freedom in the ﬁt is indicated below each plot.\\nvariate regression coeﬃcients1was 9, 33 and 331, respectively, averaged\\nover the 100 simulation runs. The p= 1000 case is designed to mimic the'),\n",
       " Document(id='c352507b-31e5-4da2-bf00-03cced94f252', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 800, 'score': 0.6213172417016296}, page_content='to eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_hf = vectorstore_hf.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs = {'k':5}\n",
    ")\n",
    "\n",
    "retriever_hf.invoke(user_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "404c808d-791a-4842-ab71-41623abe4013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2a7fa81b-0bb3-4d4c-86be-18c1bbf57590', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 760, 'score': 0.726056829878058}, page_content='over the 100 simulation runs. The p= 1000 case is designed to mimic the\\nkind of data that we might see in a high-dimensional genomic o r proteomic\\ndataset, for example.\\nWe ﬁt a ridge regression to the data, with three diﬀerent valu es for the\\nregularization parameter λ: 0.001, 100, and 1000. When λ= 0.001, this\\nis nearly the same as least squares regression, with a little regularization\\njust to ensure that the problem is non-singular when p > N. Figure 18.1')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_hf = vectorstore_hf.as_retriever(\n",
    "    # search_type='similarity_score_threshold',\n",
    "    # search_kwargs={'score_threshold': 0.0}\n",
    "    search_kwargs={'k':1}\n",
    ")\n",
    "\n",
    "retriever_hf.invoke(user_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e586de50-9ba0-4cc2-b19f-47003c292c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_hf = vectorstore_hf.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs = {'k':5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3af5eab7-3fbd-469d-92a9-4c0b558aa642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2a7fa81b-0bb3-4d4c-86be-18c1bbf57590', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 760, 'score': 0.726056829878058}, page_content='over the 100 simulation runs. The p= 1000 case is designed to mimic the\\nkind of data that we might see in a high-dimensional genomic o r proteomic\\ndataset, for example.\\nWe ﬁt a ridge regression to the data, with three diﬀerent valu es for the\\nregularization parameter λ: 0.001, 100, and 1000. When λ= 0.001, this\\nis nearly the same as least squares regression, with a little regularization\\njust to ensure that the problem is non-singular when p > N. Figure 18.1'),\n",
       " Document(id='ecd4bf7b-fd87-491b-8fb7-4ab23676d431', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 1150, 'score': 0.6877578348106839}, page_content='just to ensure that the problem is non-singular when p > N. Figure 18.1\\nshows boxplots oftherelative testerrorachieved bythediﬀ erentestimators\\nin each scenario. The corresponding average degrees of free dom used in\\neach ridge-regression ﬁt is indicated (computed using form ula (3.50) on\\npage 682). The degrees of freedom is a more interpretable parameter t han\\nλ. We see that ridge regression with λ= 0.001 (20 df) wins when p= 20;\\nλ= 100 (35 df) wins when p= 100, and λ= 1000 (43 df) wins when'),\n",
       " Document(id='0c0b3e98-2590-496a-9c54-02074cec6d03', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 397, 'score': 0.5927353881959819}, page_content='using the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,'),\n",
       " Document(id='5dce10df-48ea-4ffd-805d-027bb5e4c7b1', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 368, 'score': 0.5909602438724204}, page_content='ofp, the number of features. The relative error is the test error d ivided by the\\nBayes error, σ2. From left to right, results are shown for ridge regression wit h\\nthree diﬀerent values of the regularization parameter λ:0.001,100and1000. The\\n(average) eﬀective degrees of freedom in the ﬁt is indicated below each plot.\\nvariate regression coeﬃcients1was 9, 33 and 331, respectively, averaged\\nover the 100 simulation runs. The p= 1000 case is designed to mimic the'),\n",
       " Document(id='c352507b-31e5-4da2-bf00-03cced94f252', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 800, 'score': 0.6213172417016296}, page_content='to eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-')]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_hf.invoke(user_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a3f4e423-e3fc-4cc9-ab87-49755ace2eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2a7fa81b-0bb3-4d4c-86be-18c1bbf57590', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 760, 'score': 0.726056829878058}, page_content='over the 100 simulation runs. The p= 1000 case is designed to mimic the\\nkind of data that we might see in a high-dimensional genomic o r proteomic\\ndataset, for example.\\nWe ﬁt a ridge regression to the data, with three diﬀerent valu es for the\\nregularization parameter λ: 0.001, 100, and 1000. When λ= 0.001, this\\nis nearly the same as least squares regression, with a little regularization\\njust to ensure that the problem is non-singular when p > N. Figure 18.1'),\n",
       " Document(id='ecd4bf7b-fd87-491b-8fb7-4ab23676d431', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 1150, 'score': 0.6877578348106839}, page_content='just to ensure that the problem is non-singular when p > N. Figure 18.1\\nshows boxplots oftherelative testerrorachieved bythediﬀ erentestimators\\nin each scenario. The corresponding average degrees of free dom used in\\neach ridge-regression ﬁt is indicated (computed using form ula (3.50) on\\npage 682). The degrees of freedom is a more interpretable parameter t han\\nλ. We see that ridge regression with λ= 0.001 (20 df) wins when p= 20;\\nλ= 100 (35 df) wins when p= 100, and λ= 1000 (43 df) wins when'),\n",
       " Document(id='0c0b3e98-2590-496a-9c54-02074cec6d03', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 397, 'score': 0.5927353881959819}, page_content='using the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,'),\n",
       " Document(id='5dce10df-48ea-4ffd-805d-027bb5e4c7b1', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 368, 'score': 0.5909602438724204}, page_content='ofp, the number of features. The relative error is the test error d ivided by the\\nBayes error, σ2. From left to right, results are shown for ridge regression wit h\\nthree diﬀerent values of the regularization parameter λ:0.001,100and1000. The\\n(average) eﬀective degrees of freedom in the ﬁt is indicated below each plot.\\nvariate regression coeﬃcients1was 9, 33 and 331, respectively, averaged\\nover the 100 simulation runs. The p= 1000 case is designed to mimic the'),\n",
       " Document(id='c352507b-31e5-4da2-bf00-03cced94f252', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 800, 'score': 0.6213172417016296}, page_content='to eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_hf.invoke(user_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27753c69-6306-482e-9e2a-48e53430a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_hf.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d5e5417d-be34-4988-be66-5dfe0ba72555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def get_retrieved_documents_with_scores(vectorstore,query,k) -> List[Document]:\n",
    "    \"\"\"\n",
    "    vectorstore: vectorstore object\n",
    "    query (str): user input query\n",
    "    k (int): number of top K documents to return\n",
    "    \n",
    "    manual function to get retrieved documents from vectorstore and add score to metadata.\n",
    "    output: List of Document objects.\n",
    "    \"\"\"\n",
    "    docs, scores = zip(*vectorstore.similarity_search_with_score(query=query,k=k))\n",
    "    for doc, score in zip(docs, scores):\n",
    "        doc.metadata[\"score\"] = score\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4b688341-059f-46d3-a969-36ac5a0d0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_retrieved_documents_with_scores(vectorstore_hf,user_input1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1d1b8eac-e212-4b44-bfb4-1919e9291cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(id='2a7fa81b-0bb3-4d4c-86be-18c1bbf57590', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 760, 'score': 0.726056829878058}, page_content='over the 100 simulation runs. The p= 1000 case is designed to mimic the\\nkind of data that we might see in a high-dimensional genomic o r proteomic\\ndataset, for example.\\nWe ﬁt a ridge regression to the data, with three diﬀerent valu es for the\\nregularization parameter λ: 0.001, 100, and 1000. When λ= 0.001, this\\nis nearly the same as least squares regression, with a little regularization\\njust to ensure that the problem is non-singular when p > N. Figure 18.1'),\n",
       " Document(id='ecd4bf7b-fd87-491b-8fb7-4ab23676d431', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 1150, 'score': 0.6877578348106839}, page_content='just to ensure that the problem is non-singular when p > N. Figure 18.1\\nshows boxplots oftherelative testerrorachieved bythediﬀ erentestimators\\nin each scenario. The corresponding average degrees of free dom used in\\neach ridge-regression ﬁt is indicated (computed using form ula (3.50) on\\npage 682). The degrees of freedom is a more interpretable parameter t han\\nλ. We see that ridge regression with λ= 0.001 (20 df) wins when p= 20;\\nλ= 100 (35 df) wins when p= 100, and λ= 1000 (43 df) wins when'),\n",
       " Document(id='0c0b3e98-2590-496a-9c54-02074cec6d03', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 397, 'score': 0.5927353881959819}, page_content='using the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,'),\n",
       " Document(id='5dce10df-48ea-4ffd-805d-027bb5e4c7b1', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 1, 'start_index': 368, 'score': 0.5909602438724204}, page_content='ofp, the number of features. The relative error is the test error d ivided by the\\nBayes error, σ2. From left to right, results are shown for ridge regression wit h\\nthree diﬀerent values of the regularization parameter λ:0.001,100and1000. The\\n(average) eﬀective degrees of freedom in the ﬁt is indicated below each plot.\\nvariate regression coeﬃcients1was 9, 33 and 331, respectively, averaged\\nover the 100 simulation runs. The p= 1000 case is designed to mimic the'),\n",
       " Document(id='c352507b-31e5-4da2-bf00-03cced94f252', metadata={'source': '/Users/i748920/Desktop/llms-learning/pdf-chatbot-app/data/short-elements-of-statistical-learning-book/chap18 copy.pdf', 'page': 2, 'start_index': 800, 'score': 0.5561141467832593}, page_content='to eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-'))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a97dd9-ea14-4e88-a40d-a9fc1597d56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ec4079c-212c-4abc-9672-daf06946456c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# attempt to use manual query refiner llm instead of history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "a68c8923-3bfa-4251-ae31-56d1869ab755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# 1. setup conversation history\n",
    "\n",
    "chat_history_store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "d9df8735-da38-479b-836a-85e27fe303a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_id = \"2\"\n",
    "current_chat_history = get_session_history(session_id)\n",
    "current_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "abc07e54-c86e-4738-be43-92303555d67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='hi do you know what pythagoras theorem is? just say yes or no'), AIMessage(content='Yes. What would you like to know about it?')])"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add some messages\n",
    "\n",
    "current_chat_history.add_user_message(\"hi do you know what pythagoras theorem is? just say yes or no\")\n",
    "current_chat_history.add_ai_message(\"Yes. What would you like to know about it?\")\n",
    "current_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "3d72c481-65cb-4bae-b0b4-04ba920d90ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='hi do you know what pythagoras theorem is? just say yes or no'), AIMessage(content='Yes. What would you like to know about it?')])"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_history(\"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16137d52-9587-4119-9849-29485b2e7bc0",
   "metadata": {},
   "source": [
    "history_aware_retriever\n",
    "\n",
    "* Create a chain that takes conversation history and returns documents.\n",
    "* If there is no chat_history, then the input is just passed directly to the retriever. If there is chat_history, then the prompt and LLM will be used to generate a search query. That search query is then passed to the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "c8d69d39-1a5a-4df1-b3a2-aa314871ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup llm model for refining the user input with conversation context\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "query_refiner_llm_model_name = \"llama3.1\"\n",
    "query_refiner_llm = ChatOllama(\n",
    "    model=query_refiner_llm_model_name,\n",
    "    temperature=0 # increase temp for more creative answers\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "6179ac94-cc43-45a4-903f-e7c2a90e4e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"How's it going? Is there something I can help you with or would you like to chat?\", response_metadata={'model': 'llama3.1', 'created_at': '2024-09-18T05:46:38.621339Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1068434333, 'load_duration': 28997833, 'prompt_eval_count': 11, 'prompt_eval_duration': 310055000, 'eval_count': 21, 'eval_duration': 728042000}, id='run-917dec4c-7b87-4d65-a426-54d0ae073f47-0', usage_metadata={'input_tokens': 11, 'output_tokens': 21, 'total_tokens': 32})"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "query_refiner_llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "2feafdcf-ec5c-451b-8581-703803cf6533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='hi do you know what pythagoras theorem is? just say yes or no'), AIMessage(content='Yes. What would you like to know about it?')])"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "9dd5d5f3-4afb-4d38-8162-0fa8990147b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_input_query_with_chat_history_context(session_id, input_query, query_refiner_llm):\n",
    "    \"\"\"\n",
    "    session_id is tyope str - the sessions_id of the chat history\n",
    "    input_query is type str - input in the terminal by the user\n",
    "    query_refiner_llm is a base chat LLM used to refine the input query with context of the chat history\n",
    "    \"\"\"\n",
    "    chat_history = get_session_history(session_id)\n",
    "\n",
    "    # Setup system contextualise input prompt\n",
    "    system_contextualize_input_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "    \n",
    "    # Prepare the chat prompt template with the chat history\n",
    "    system_input_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_contextualize_input_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),  # For history\n",
    "            (\"human\", \"{input}\"),  # User's latest input query\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Format the chat history as a list of dictionaries (required for MessagesPlaceholder)\n",
    "    formatted_chat_history = []\n",
    "    for message in chat_history.messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            formatted_chat_history.append({\"role\": \"human\", \"content\": message.content})\n",
    "        elif isinstance(message, AIMessage):\n",
    "            formatted_chat_history.append({\"role\": \"ai\", \"content\": message.content})\n",
    "\n",
    "    # Invoke the template with chat history and input query\n",
    "    system_input_prompt_value = system_input_prompt_template.invoke(\n",
    "        {\n",
    "            'chat_history': formatted_chat_history,  # Pass the formatted history\n",
    "            'input': input_query  # User's current query\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"chat history:\")\n",
    "    print(system_input_prompt_value.messages)\n",
    "    \n",
    "    # Use the query_refiner_llm to get the refined_query\n",
    "    refined_query = query_refiner_llm.invoke(\n",
    "        input = system_input_prompt_value.messages\n",
    "    )\n",
    "    \n",
    "    return refined_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "d4babbe8-1be0-48e6-a9c3-726e3894789a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat history:\n",
      "[SystemMessage(content='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), HumanMessage(content='hi do you know what pythagoras theorem is? just say yes or no'), AIMessage(content='Yes. What would you like to know about it?'), HumanMessage(content='okay explain it.')]\n"
     ]
    }
   ],
   "source": [
    "refined_query =  refine_input_query_with_chat_history_context(\n",
    "    session_id='2',\n",
    "    input_query=\"okay explain it.\",\n",
    "    query_refiner_llm=query_refiner_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "5e8d93dd-ce68-4afd-bc6f-e43e4749b83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'),\n",
       " HumanMessage(content='hi do you know what pythagoras theorem is? just say yes or no'),\n",
       " AIMessage(content='Yes. What would you like to know about it?'),\n",
       " HumanMessage(content='okay explain it.')]"
      ]
     },
     "execution_count": 756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_input_prompt_value.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "99f5b35a-cdfd-4120-98fb-84df0c70b75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Pythagorean Theorem states that in a right-angled triangle, the square of the length of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the lengths of the other two sides.\\n\\nMathematically, this can be expressed as:\\n\\na² + b² = c²\\n\\nwhere 'a' and 'b' are the lengths of the two shorter sides, and 'c' is the length of the hypotenuse.\""
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_query.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bdbc5d-7939-494d-be6a-936c65c15518",
   "metadata": {},
   "source": [
    "issue here is that the refined_query_llm is not returning the refined query but directly answering the question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772dfa33-071f-4716-9d90-0eb1029803d4",
   "metadata": {},
   "source": [
    "## others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ab857-5a70-4843-938d-82565546b76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "5e7cb332-24e7-4d03-ab9c-3efbb1326448",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chat_history.add_user_message(\"hi can you explain pythagoras theorem to me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "c2f3116d-bc28-4c7c-b7eb-e90884490610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='hi can you explain pythagoras theorem to me?')])"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "63268123-2258-478b-80e7-26c76232f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chat_history.add_ai_message(\"i cant find information on that in the vector db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "bb90d919-6c7c-4977-8843-ccaac75e0269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi can you explain pythagoras theorem to me?'),\n",
       " AIMessage(content='i cant find information on that in the vector db')]"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070c087-06bc-4118-b554-18ea14f03aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bffb8-4edc-41bc-b24f-173653cb415a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "dc1bd72d-76fc-43f7-8d6c-f5c2f8ffe683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "def refine_query_with_context(input_query, session_id, query_refiner_llm):\n",
    "    \"\"\"\n",
    "    Refines the input query by considering the chat history.\n",
    "    \"\"\"\n",
    "    # Get the chat history for the session (assume you have a function to fetch it)\n",
    "    chat_history = get_session_history(session_id)\n",
    "\n",
    "    # Setup system contextualize input prompt\n",
    "    system_contextualize_input_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "\n",
    "    # Prepare the chat prompt template with the chat history\n",
    "    system_input_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_contextualize_input_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),  # For history\n",
    "            (\"human\", \"{input}\"),  # User's latest input\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Format the chat history as a list of dictionaries (required for MessagesPlaceholder)\n",
    "    formatted_history = []\n",
    "    for message in chat_history.messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            formatted_history.append({\"role\": \"human\", \"content\": message.content})\n",
    "        elif isinstance(message, AIMessage):\n",
    "            formatted_history.append({\"role\": \"ai\", \"content\": message.content})\n",
    "\n",
    "    # Invoke the template with chat history and input query\n",
    "    refined_query = system_input_prompt_template.invoke(\n",
    "        {\n",
    "            'chat_history': formatted_history,  # Pass the formatted history\n",
    "            'input': input_query  # User's current query\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return refined_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "f8c6d017-f23c-48cd-8cc3-3e82e3592bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_query = refine_query_with_context(input_query=sample_query,session_id='1',query_refiner_llm=query_refiner_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "170cd807-2132-4108-9ad0-4a851b5f8f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'),\n",
       " HumanMessage(content='hi can you explain pythagoras theorem to me?'),\n",
       " AIMessage(content='i cant find information on that in the vector db'),\n",
       " HumanMessage(content='for high-dimensional problems, with regards to p and N, in what cases can ridge regression exploit the correlation in the features of the dataset?')]"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_query.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "f14b52e0-d395-4f66-9280-1df676e142db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is Ridge Regression's ability to exploit feature correlations in high-dimensional spaces when considering p (number of features) and N (sample size)?\""
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_refiner_llm.invoke(input=refined_query)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae3044-caec-4bcc-8202-0b00c9c02328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca48a6-5d67-4f81-918c-05fd69d0d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_refiner_llm.invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "3616c639-489f-4417-95d3-f4df655d5904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "variable chat_history should be a list of base messages, got Human: hi can you explain pythagoras theorem to me?\nAI: i cant find information on that in the vector db of type <class 'langchain_core.chat_history.InMemoryChatMessageHistory'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[681], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m system_input_prompt_temmplate \u001b[38;5;241m=\u001b[39m \u001b[43mrefine_query_with_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43msession_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mquery_refiner_llm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_refiner_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m system_input_prompt_temmplate\n",
      "Cell \u001b[0;32mIn[679], line 24\u001b[0m, in \u001b[0;36mrefine_query_with_context\u001b[0;34m(input_query, session_id, query_refiner_llm)\u001b[0m\n\u001b[1;32m      8\u001b[0m system_contextualise_input_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven a chat history and the latest user question \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich might reference context in the chat history, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjust reformulate it if needed and otherwise return it as is.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m system_input_prompt_temmplate \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     16\u001b[0m     [\n\u001b[1;32m     17\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, system_contextualise_input_prompt),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     ]\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m system_input_prompt \u001b[38;5;241m=\u001b[39m \u001b[43msystem_input_prompt_temmplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43minput_query\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m system_input_prompt_temmplate\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/prompts/base.py:187\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[1;32m    186\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdumpd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py:1786\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1783\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1784\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1785\u001b[0m         Output,\n\u001b[0;32m-> 1786\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1794\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1796\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/config.py:398\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    397\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/prompts/base.py:162\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m    161\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inner_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/prompts/chat.py:765\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m    757\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03m        PromptValue.\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/prompts/chat.py:1207\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1205\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[1;32m   1206\u001b[0m ):\n\u001b[0;32m-> 1207\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mmessage_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/prompts/chat.py:231\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    226\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name, [])\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name]\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be a list of base messages, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    234\u001b[0m     )\n\u001b[1;32m    235\u001b[0m value \u001b[38;5;241m=\u001b[39m convert_to_messages(value)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_messages:\n",
      "\u001b[0;31mValueError\u001b[0m: variable chat_history should be a list of base messages, got Human: hi can you explain pythagoras theorem to me?\nAI: i cant find information on that in the vector db of type <class 'langchain_core.chat_history.InMemoryChatMessageHistory'>"
     ]
    }
   ],
   "source": [
    "system_input_prompt_temmplate = refine_query_with_context(input_query=sample_query,session_id='1',query_refiner_llm=query_refiner_llm)\n",
    "system_input_prompt_temmplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e9037-cc98-4989-8257-94f972fb3999",
   "metadata": {},
   "source": [
    "build entire pipeline in order without langchain as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bd984-fe11-4546-9e6b-9b13bd513a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "    # setup llm model for refining the user input with conversation context\n",
    "    prompt_refiner_llm = ChatOllama(\n",
    "        model=llm_model_name,\n",
    "        temperature=0 # increase temp for more creative answers\n",
    "    ) \n",
    "    \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "8f72ca12-8e2e-4d91-a750-b43de69d9679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'runnable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[571], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m         store[session_id] \u001b[38;5;241m=\u001b[39m ChatMessageHistory()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m store[session_id]\n\u001b[1;32m     14\u001b[0m with_message_history \u001b[38;5;241m=\u001b[39m RunnableWithMessageHistory(\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mrunnable\u001b[49m,\n\u001b[1;32m     16\u001b[0m     get_session_history,\n\u001b[1;32m     17\u001b[0m     input_messages_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     history_messages_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'runnable' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
