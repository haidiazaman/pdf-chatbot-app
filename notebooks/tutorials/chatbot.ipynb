{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e6dcd3-62c2-416b-9afe-477028a03a39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "424e6d8f-ff59-4406-8749-49360e965a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7873c848-af26-405f-8820-a4017a30f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Fourier series is a mathematical tool used in analysis, specifically in the study of periodic functions. It is named after Jean-Baptiste Joseph Fourier, who introduced the concept in his study of heat transfer.\n",
      "\n",
      "The Fourier series is a way to represent a function as a sum of simple sine and cosine waves. More specifically, it decomposes any periodic function or periodic signal into the sum of a set of simple oscillating functions, namely sines and cosines (or complex exponentials).\n",
      "\n",
      "This concept is particularly useful in solving partial differential equations, in signal processing, and in data compression. It is also widely used in physics and engineering, as it helps to analyze and simplify complex systems with periodic behaviors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Laminar flow, also known as streamline flow, is a type of flow pattern of a fluid in which all the particles are flowing in parallel lines and in the same direction, without any disruption or cross currents. In this flow pattern, the fluid flows smoothly or in regular paths, in contrast to turbulent flow which is characterized by chaotic, irregular fluid motions.\\n\\nLaminar flow occurs at lower speeds and high viscosity where the fluid particles can flow without interruption. It\\'s a fundamental concept in fluid dynamics and is used in various scientific and engineering applications, such as in the design of aircrafts and cars for smooth airflow, or in the medical field for blood flow through blood vessels. \\n\\nThe term \"laminar flow\" comes from the word \"lamina\", which means a layer. Hence, in laminar flow, the flow of fluid is imagined as layers sliding over one another. Each layer has a definite velocity and moves smoothly over the adjoining layer.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model using concept from previous langchain_basics notebook\n",
    "\n",
    "from langchain_core.messages import SystemMessage,HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Explain the following mathematical concept.\"),\n",
    "    HumanMessage(content=\"Fourier Series\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "parsed_output = parser.invoke(result)\n",
    "print(parsed_output)\n",
    "\n",
    "# using chain - prompt template - model - output parser\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = 'explain the following {topic} concept' \n",
    "human_template = '{text}'\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "        ('system',system_template),\n",
    "        ('user',human_template)\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\n",
    "    'topic':'science',\n",
    "    'text':'metabolism'\n",
    "})\n",
    "\n",
    "prompt.to_messages()\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "result = chain.invoke({\n",
    "    'topic':'science',\n",
    "    'text':'laminar flow'\n",
    "})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f1e872-5fe8-481d-8048-652982a5fd11",
   "metadata": {},
   "source": [
    "# Build Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e1101-9197-4bdc-86e4-50a91c316657",
   "metadata": {},
   "source": [
    "Based on the tutorial https://python.langchain.com/v0.2/docs/tutorials/chatbot/\n",
    "you will need to continuously add the system's response to create a chatbot, else the chatbot wont rmb what has been mentioned before, thus there is a need to implement MessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656ead6-b331-4b28-b913-6a0b66e93d6c",
   "metadata": {},
   "source": [
    "get_session_history. This function is expected to take in a session_id and return a Message History object. This session_id is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain (we'll show how to do that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e18dcf56-b781-4889-8e98-e90699f99e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1443d99a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1443d4e50>, root_client=<openai.OpenAI object at 0x14436b310>, root_async_client=<openai.AsyncOpenAI object at 0x1443d9a00>, model_name='gpt-4', openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36f02491-e9e7-447e-8e7c-e324e1df2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# create store\n",
    "store  = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        # create new chat history\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# create a model with message history \n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)\n",
    "# - can pass in a chain with a model inside as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a8093-3c83-4c43-9c6f-d61cafb4717d",
   "metadata": {},
   "source": [
    "We now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a session_id. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb9ec010-5708-4104-aba3-8a2794b12c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'configurable':{\n",
    "        'session_id':'abc2'\n",
    "    }\n",
    "}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content='Whats my name?')\n",
    "    ],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed2687-fa8c-4294-a0b5-ed19f47d25a9",
   "metadata": {},
   "source": [
    "^ because havent provide the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bfbce31d-2970-4956-81f1-a14b9802cf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Dee! How can I assist you today?'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'configurable':{\n",
    "        'session_id':'abc2'\n",
    "    }\n",
    "}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content='Hi my name is dee!')\n",
    "    ],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a2eaa3a-e5bd-4bbd-86c0-e1b1dd8075b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([HumanMessage(content='Whats my name?'),\n",
       "  AIMessage(content=\"As an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 11, 'total_tokens': 49}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1c891908-3dec-49b1-a821-bacb977fb460-0', usage_metadata={'input_tokens': 11, 'output_tokens': 38, 'total_tokens': 49}),\n",
       "  HumanMessage(content='Hi my name is dee!'),\n",
       "  AIMessage(content='Nice to meet you, Dee! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 63, 'total_tokens': 77}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d70de965-f831-4d40-a206-306dc39a5023-0', usage_metadata={'input_tokens': 63, 'output_tokens': 14, 'total_tokens': 77})],\n",
       " 'Whats my name?')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check message history\n",
    "\n",
    "store['abc2'].messages,store['abc2'].messages[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a7ec8eb-44b0-4949-8ff0-06b343eac5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Dee.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'configurable':{\n",
    "        'session_id':'abc2'\n",
    "    }\n",
    "}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content='Whats my name now?')\n",
    "    ],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e2f96ad-f249-4f48-9ffa-458c5da8e816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an artificial intelligence, I'm not able to know your name unless you tell me.\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if use a different session_id name, then the chat refers to a new conversation with new message history\n",
    "\n",
    "config2 = {\n",
    "    'configurable':{\n",
    "        'session_id':'abc4'\n",
    "    }\n",
    "}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    HumanMessage(\"whats my name?\"),\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08ea86-b7e7-41c7-af97-f6200f1af8bf",
   "metadata": {},
   "source": [
    "pass chain instead of model in RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f34a8c8a-4831-43c2-8a29-4dbbb04d0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "prompts = ChatPromptTemplate.from_messages([\n",
    "    ('system','You are a helpful personal assistant. Answer all questions to the best of your abilities.'),\n",
    "    MessagesPlaceholder(variable_name='messages')\n",
    "])\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1dd69c3e-2a9e-4634-8c42-1e0df2078af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but as an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        'messages':[\n",
    "            HumanMessage(content='hi! what is my name?')\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb9a96-6e3b-4548-b7db-2c46121ce083",
   "metadata": {},
   "source": [
    "now wrap this in the runnable message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5cefaeea-f2a4-49df-a930-e95aececbfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Alex. How can I assist you today?'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)\n",
    "config = {\n",
    "    'configurable':{\n",
    "        'session_id':'abc'\n",
    "    }\n",
    "}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content='hi whats my name?')\n",
    "    ],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f1528ba7-f3a0-4fc4-82a7-7bf3d7e0d69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Alex! How can I assist you today?'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content='my name is alex')\n",
    "    ],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e101fd4f-be12-4c80-a6e2-4b37d2ec7f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Alex. How can I help you today?'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content='whats my name?')\n",
    "    ],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a4c2ca40-e3ad-4aff-aa81-44b5b0c29519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sistem suria adalah kumpulan planet, asteroid, meteor, komet dan benda angkasa lain yang mengorbit matahari. Ia termasuk planet seperti Merkurius, Venus, Bumi, Mars, Jupiter, Saturnus, Uranus, dan Neptunus.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add language as input\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are a helpful assistant. answer the questions in {language} please.'),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, \n",
    "    get_session_history,\n",
    "    input_messages_key='messages'\n",
    ")\n",
    "\n",
    "config = {\n",
    "    'configurable':{\n",
    "        'session_id':'abc6'\n",
    "    }\n",
    "}\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        'messages':[HumanMessage(content='what is the solar system?')],\n",
    "        'language': \"Malay\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2177787-dd7f-427e-9a10-8ae0ef2423fe",
   "metadata": {},
   "source": [
    "# Managing Conversation History\n",
    "\n",
    "* there is a need to ensure that the history doesnt exceed the context window of the LLM\n",
    "* BEFORE the prompt template but AFTER you load previous messages from Message History."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0d8929fa-1eab-4133-9518-edd045e90857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content='whats 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content='no problem!'),\n",
       " HumanMessage(content='having fun?'),\n",
       " AIMessage(content='yes!')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, AIMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n",
    "# only keeps the last messages that fits the max 65 tokens + keep system message and start on human message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5a8daff6-9240-4c4a-8d99-d13f77100db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but as an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8d423352-4b66-42a3-9bed-bcfadec7a50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but as an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\""
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc10\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0866bcb-982a-48c7-ba37-80a0b5278e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
