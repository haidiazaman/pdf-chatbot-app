{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word / sentence tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word tokenization using nltk and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'Product Allocation (PAL) in advanced Available-to-Promise (aATP) is a mechanism in SAP S/4HANA that helps avoid critical situations in demand and procurement. It allows the allocation of materials in short supply to specific regions and customers for a specific time period. This ensures that the entire available quantity of a material is not allocated to a single customer, enabling subsequent order requirements from other customers to be confirmed. PAL helps in precise planning and control of material delivery to meet customer demands.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/I748920/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Product',\n",
       " 'Allocation',\n",
       " '(',\n",
       " 'PAL',\n",
       " ')',\n",
       " 'in',\n",
       " 'advanced',\n",
       " 'Available-to-Promise',\n",
       " '(',\n",
       " 'aATP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'mechanism',\n",
       " 'in',\n",
       " 'SAP',\n",
       " 'S/4HANA',\n",
       " 'that',\n",
       " 'helps',\n",
       " 'avoid',\n",
       " 'critical',\n",
       " 'situations',\n",
       " 'in',\n",
       " 'demand',\n",
       " 'and',\n",
       " 'procurement',\n",
       " '.',\n",
       " 'It',\n",
       " 'allows',\n",
       " 'the',\n",
       " 'allocation',\n",
       " 'of',\n",
       " 'materials',\n",
       " 'in',\n",
       " 'short',\n",
       " 'supply',\n",
       " 'to',\n",
       " 'specific',\n",
       " 'regions',\n",
       " 'and',\n",
       " 'customers',\n",
       " 'for',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'time',\n",
       " 'period',\n",
       " '.',\n",
       " 'This',\n",
       " 'ensures',\n",
       " 'that',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'available',\n",
       " 'quantity',\n",
       " 'of',\n",
       " 'a',\n",
       " 'material',\n",
       " 'is',\n",
       " 'not',\n",
       " 'allocated',\n",
       " 'to',\n",
       " 'a',\n",
       " 'single',\n",
       " 'customer',\n",
       " ',',\n",
       " 'enabling',\n",
       " 'subsequent',\n",
       " 'order',\n",
       " 'requirements',\n",
       " 'from',\n",
       " 'other',\n",
       " 'customers',\n",
       " 'to',\n",
       " 'be',\n",
       " 'confirmed',\n",
       " '.',\n",
       " 'PAL',\n",
       " 'helps',\n",
       " 'in',\n",
       " 'precise',\n",
       " 'planning',\n",
       " 'and',\n",
       " 'control',\n",
       " 'of',\n",
       " 'material',\n",
       " 'delivery',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'customer',\n",
       " 'demands',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk - word tokenization\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "word_tokenize(sample_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I748920/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\n",
      "Allocation\n",
      "(\n",
      "PAL\n",
      ")\n",
      "in\n",
      "advanced\n",
      "Available\n",
      "-\n",
      "to\n",
      "-\n",
      "Promise\n",
      "(\n",
      "aATP\n",
      ")\n",
      "is\n",
      "a\n",
      "mechanism\n",
      "in\n",
      "SAP\n",
      "S/4HANA\n",
      "that\n",
      "helps\n",
      "avoid\n",
      "critical\n",
      "situations\n",
      "in\n",
      "demand\n",
      "and\n",
      "procurement\n",
      ".\n",
      "It\n",
      "allows\n",
      "the\n",
      "allocation\n",
      "of\n",
      "materials\n",
      "in\n",
      "short\n",
      "supply\n",
      "to\n",
      "specific\n",
      "regions\n",
      "and\n",
      "customers\n",
      "for\n",
      "a\n",
      "specific\n",
      "time\n",
      "period\n",
      ".\n",
      "This\n",
      "ensures\n",
      "that\n",
      "the\n",
      "entire\n",
      "available\n",
      "quantity\n",
      "of\n",
      "a\n",
      "material\n",
      "is\n",
      "not\n",
      "allocated\n",
      "to\n",
      "a\n",
      "single\n",
      "customer\n",
      ",\n",
      "enabling\n",
      "subsequent\n",
      "order\n",
      "requirements\n",
      "from\n",
      "other\n",
      "customers\n",
      "to\n",
      "be\n",
      "confirmed\n",
      ".\n",
      "PAL\n",
      "helps\n",
      "in\n",
      "precise\n",
      "planning\n",
      "and\n",
      "control\n",
      "of\n",
      "material\n",
      "delivery\n",
      "to\n",
      "meet\n",
      "customer\n",
      "demands\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# spacy - word tokenization\n",
    "\n",
    "import spacy\n",
    "# run this first\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for i in nlp(sample_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Product, Allocation, (, PAL)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(sample_text)[0],nlp(sample_text)[1],nlp(sample_text)[2],nlp(sample_text)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for word tokenization, either nltk or spacy libraries work fine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Product Allocation (PAL) in advanced Available-to-Promise (aATP) is a mechanism in SAP S/4HANA that helps avoid critical situations in demand and procurement.',\n",
       " 'It allows the allocation of materials in short supply to specific regions and customers for a specific time period.',\n",
       " 'This ensures that the entire available quantity of a material is not allocated to a single customer, enabling subsequent order requirements from other customers to be confirmed.',\n",
       " 'PAL helps in precise planning and control of material delivery to meet customer demands.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk - sentence tokenization\n",
    "\n",
    "nltk_sentence_tokens = nltk.sent_tokenize(sample_text)\n",
    "nltk_sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Allocation (PAL) in advanced Available-to-Promise (aATP) is a mechanism in SAP S/4HANA that helps avoid critical situations in demand and procurement. \n",
      "\n",
      "It allows the allocation of materials in short supply to specific regions and customers for a specific time period. \n",
      "\n",
      "This ensures that the entire available quantity of a material is not allocated to a single customer, enabling subsequent order requirements from other customers to be confirmed. \n",
      "\n",
      "PAL helps in precise planning and control of material delivery to meet customer demands. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nltk - sentence tokenization\n",
    "\n",
    "spacy_sentence_tokens = nlp(sample_text).sents\n",
    "for i in spacy_sentence_tokens:\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for sentence tokenization\n",
    "Sentence tokenization takes a text and splits it into individual sentences. For literature, journalism, and formal documents the tokenization algorithms built into spaCy perform well, since the tokenizer is trained on a corpus of formal English text. The sentence tokenizer shows poor performance for electronic health records featuring abbreviations, medical terms, spatial measurements, and other forms not present in standard written English.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk seems to be better for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Stop Words:\n",
    "StopWords are English words that do not add much meaning to a sentence, so we can remove all the stop words from the text. E.g. “a”, ”the”, ”have”, ”an” etc…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK data package includes a pre-trained Punkt tokenizer for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Product Allocation (PAL) in advanced Available-to-Promise (aATP) is a mechanism in SAP S/4HANA that helps avoid critical situations in demand and procurement. It allows the allocation of materials in short supply to specific regions and customers for a specific time period. This ensures that the entire available quantity of a material is not allocated to a single customer, enabling subsequent order requirements from other customers to be confirmed. PAL helps in precise planning and control of material delivery to meet customer demands.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/I748920/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/I748920/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk - stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Product',\n",
       " 'Allocation',\n",
       " '(PAL)',\n",
       " 'advanced',\n",
       " 'Available-to-Promise',\n",
       " '(aATP)',\n",
       " 'mechanism',\n",
       " 'SAP',\n",
       " 'S/4HANA',\n",
       " 'helps',\n",
       " 'avoid',\n",
       " 'critical',\n",
       " 'situations',\n",
       " 'demand',\n",
       " 'procurement.',\n",
       " 'It',\n",
       " 'allows',\n",
       " 'allocation',\n",
       " 'materials',\n",
       " 'short',\n",
       " 'supply',\n",
       " 'specific',\n",
       " 'regions',\n",
       " 'customers',\n",
       " 'specific',\n",
       " 'time',\n",
       " 'period.',\n",
       " 'This',\n",
       " 'ensures',\n",
       " 'entire',\n",
       " 'available',\n",
       " 'quantity',\n",
       " 'material',\n",
       " 'allocated',\n",
       " 'single',\n",
       " 'customer,',\n",
       " 'enabling',\n",
       " 'subsequent',\n",
       " 'order',\n",
       " 'requirements',\n",
       " 'customers',\n",
       " 'confirmed.',\n",
       " 'PAL',\n",
       " 'helps',\n",
       " 'precise',\n",
       " 'planning',\n",
       " 'control',\n",
       " 'material',\n",
       " 'delivery',\n",
       " 'meet',\n",
       " 'customer',\n",
       " 'demands.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[]\n",
    "for i in sample_text.split():\n",
    "    if i not in stopwords.words('english'):\n",
    "        x.append(i)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spacy - stop words\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "spacy_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Product',\n",
       " 'Allocation',\n",
       " '(PAL)',\n",
       " 'advanced',\n",
       " 'Available-to-Promise',\n",
       " '(aATP)',\n",
       " 'mechanism',\n",
       " 'SAP',\n",
       " 'S/4HANA',\n",
       " 'helps',\n",
       " 'avoid',\n",
       " 'critical',\n",
       " 'situations',\n",
       " 'demand',\n",
       " 'procurement.',\n",
       " 'It',\n",
       " 'allows',\n",
       " 'allocation',\n",
       " 'materials',\n",
       " 'short',\n",
       " 'supply',\n",
       " 'specific',\n",
       " 'regions',\n",
       " 'customers',\n",
       " 'specific',\n",
       " 'time',\n",
       " 'period.',\n",
       " 'This',\n",
       " 'ensures',\n",
       " 'entire',\n",
       " 'available',\n",
       " 'quantity',\n",
       " 'material',\n",
       " 'allocated',\n",
       " 'single',\n",
       " 'customer,',\n",
       " 'enabling',\n",
       " 'subsequent',\n",
       " 'order',\n",
       " 'requirements',\n",
       " 'customers',\n",
       " 'confirmed.',\n",
       " 'PAL',\n",
       " 'helps',\n",
       " 'precise',\n",
       " 'planning',\n",
       " 'control',\n",
       " 'material',\n",
       " 'delivery',\n",
       " 'meet',\n",
       " 'customer',\n",
       " 'demands.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[]\n",
    "for i in sample_text.split():\n",
    "    if i not in spacy_stopwords:\n",
    "        x.append(i)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the libraries, spaCy and NLTK have done a decent job in removing the stop words from the paragraph. Both can get your task done quite efficiently. However, to pick a winner, spaCy has done better in the segment which is quite accurate. Moreover, NLTK requires downloading the required package to perform the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization:\n",
    "Lemmatization is the text normalization technique for spaCy, that will remove words having the same meaning. It is a process of getting the base word of a given word i.e. “counter”, ”count”, so here the base word is “count”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization seeks to distill words to their foundational forms. In this linguistic refinement, the resultant base word is referred to as a “lemma.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization techniques in natural language processing (NLP) involve methods to identify and transform words into their base or root forms, known as lemmas. These approaches contribute to text normalization, facilitating more accurate language analysis and processing in various NLP applications. Three types of lemmatization techniques are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(spacy.tokens.token.Token, spacy.tokens.doc.Doc)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the text using spaCy\n",
    "type(nlp(sample_text)[0]),type(nlp(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Product\n",
      "Allocation Allocation\n",
      "( (\n",
      "PAL PAL\n",
      ") )\n",
      "in in\n",
      "advanced advanced\n",
      "Available Available\n",
      "- -\n",
      "to to\n",
      "- -\n",
      "Promise promise\n",
      "( (\n",
      "aATP aATP\n",
      ") )\n",
      "is be\n",
      "a a\n",
      "mechanism mechanism\n",
      "in in\n",
      "SAP SAP\n",
      "S/4HANA S/4HANA\n",
      "that that\n",
      "helps help\n",
      "avoid avoid\n",
      "critical critical\n",
      "situations situation\n",
      "in in\n",
      "demand demand\n",
      "and and\n",
      "procurement procurement\n",
      ". .\n",
      "It it\n",
      "allows allow\n",
      "the the\n",
      "allocation allocation\n",
      "of of\n",
      "materials material\n",
      "in in\n",
      "short short\n",
      "supply supply\n",
      "to to\n",
      "specific specific\n",
      "regions region\n",
      "and and\n",
      "customers customer\n",
      "for for\n",
      "a a\n",
      "specific specific\n",
      "time time\n",
      "period period\n",
      ". .\n",
      "This this\n",
      "ensures ensure\n",
      "that that\n",
      "the the\n",
      "entire entire\n",
      "available available\n",
      "quantity quantity\n",
      "of of\n",
      "a a\n",
      "material material\n",
      "is be\n",
      "not not\n",
      "allocated allocate\n",
      "to to\n",
      "a a\n",
      "single single\n",
      "customer customer\n",
      ", ,\n",
      "enabling enable\n",
      "subsequent subsequent\n",
      "order order\n",
      "requirements requirement\n",
      "from from\n",
      "other other\n",
      "customers customer\n",
      "to to\n",
      "be be\n",
      "confirmed confirm\n",
      ". .\n",
      "PAL pal\n",
      "helps help\n",
      "in in\n",
      "precise precise\n",
      "planning planning\n",
      "and and\n",
      "control control\n",
      "of of\n",
      "material material\n",
      "delivery delivery\n",
      "to to\n",
      "meet meet\n",
      "customer customer\n",
      "demands demand\n",
      ". .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Product',\n",
       " 'Allocation',\n",
       " '(',\n",
       " 'PAL',\n",
       " ')',\n",
       " 'in',\n",
       " 'advanced',\n",
       " 'Available',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'promise',\n",
       " '(',\n",
       " 'aATP',\n",
       " ')',\n",
       " 'be',\n",
       " 'a',\n",
       " 'mechanism',\n",
       " 'in',\n",
       " 'SAP',\n",
       " 'S/4HANA',\n",
       " 'that',\n",
       " 'help',\n",
       " 'avoid',\n",
       " 'critical',\n",
       " 'situation',\n",
       " 'in',\n",
       " 'demand',\n",
       " 'and',\n",
       " 'procurement',\n",
       " '.',\n",
       " 'it',\n",
       " 'allow',\n",
       " 'the',\n",
       " 'allocation',\n",
       " 'of',\n",
       " 'material',\n",
       " 'in',\n",
       " 'short',\n",
       " 'supply',\n",
       " 'to',\n",
       " 'specific',\n",
       " 'region',\n",
       " 'and',\n",
       " 'customer',\n",
       " 'for',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'time',\n",
       " 'period',\n",
       " '.',\n",
       " 'this',\n",
       " 'ensure',\n",
       " 'that',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'available',\n",
       " 'quantity',\n",
       " 'of',\n",
       " 'a',\n",
       " 'material',\n",
       " 'be',\n",
       " 'not',\n",
       " 'allocate',\n",
       " 'to',\n",
       " 'a',\n",
       " 'single',\n",
       " 'customer',\n",
       " ',',\n",
       " 'enable',\n",
       " 'subsequent',\n",
       " 'order',\n",
       " 'requirement',\n",
       " 'from',\n",
       " 'other',\n",
       " 'customer',\n",
       " 'to',\n",
       " 'be',\n",
       " 'confirm',\n",
       " '.',\n",
       " 'pal',\n",
       " 'help',\n",
       " 'in',\n",
       " 'precise',\n",
       " 'planning',\n",
       " 'and',\n",
       " 'control',\n",
       " 'of',\n",
       " 'material',\n",
       " 'delivery',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'customer',\n",
       " 'demand',\n",
       " '.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spacy - lemmatization\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "x=[]\n",
    "# for spacy need to do nlp(text) to create spacy doc / token objects\n",
    "for i in nlp(sample_text):\n",
    "    x.append(i.lemma_)\n",
    "    print(i,i.lemma_)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/I748920/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "# nltk - lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "# indicates that the word \"better\" has been lemmatized to its base form, which is \"good,\" when treated as an adjective (denoted by pos=\"a\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, and “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming: Faster, but may create unrecognizable words and lose meaning. This is known as “over stemming.” Lemmatization: More accurate, preserves meaning and grammatical function, but slower. It is often used to maintain related words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatization vs stemming\n",
    "\n",
    "https://stackoverflow.com/questions/49354665/should-i-perform-both-lemmatization-and-stemming\n",
    "https://www.datacamp.com/tutorial/stemming-lemmatization-python\n",
    "* think for now stick to lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product product\n",
      "Allocation alloc\n",
      "(PAL) (pal)\n",
      "in in\n",
      "advanced advanc\n",
      "Available-to-Promise available-to-promis\n",
      "(aATP) (aatp)\n",
      "is is\n",
      "a a\n",
      "mechanism mechan\n",
      "in in\n",
      "SAP sap\n",
      "S/4HANA s/4hana\n",
      "that that\n",
      "helps help\n",
      "avoid avoid\n",
      "critical critic\n",
      "situations situat\n",
      "in in\n",
      "demand demand\n",
      "and and\n",
      "procurement. procurement.\n",
      "It it\n",
      "allows allow\n",
      "the the\n",
      "allocation alloc\n",
      "of of\n",
      "materials materi\n",
      "in in\n",
      "short short\n",
      "supply suppli\n",
      "to to\n",
      "specific specif\n",
      "regions region\n",
      "and and\n",
      "customers custom\n",
      "for for\n",
      "a a\n",
      "specific specif\n",
      "time time\n",
      "period. period.\n",
      "This thi\n",
      "ensures ensur\n",
      "that that\n",
      "the the\n",
      "entire entir\n",
      "available avail\n",
      "quantity quantiti\n",
      "of of\n",
      "a a\n",
      "material materi\n",
      "is is\n",
      "not not\n",
      "allocated alloc\n",
      "to to\n",
      "a a\n",
      "single singl\n",
      "customer, customer,\n",
      "enabling enabl\n",
      "subsequent subsequ\n",
      "order order\n",
      "requirements requir\n",
      "from from\n",
      "other other\n",
      "customers custom\n",
      "to to\n",
      "be be\n",
      "confirmed. confirmed.\n",
      "PAL pal\n",
      "helps help\n",
      "in in\n",
      "precise precis\n",
      "planning plan\n",
      "and and\n",
      "control control\n",
      "of of\n",
      "material materi\n",
      "delivery deliveri\n",
      "to to\n",
      "meet meet\n",
      "customer custom\n",
      "demands. demands.\n"
     ]
    }
   ],
   "source": [
    "# nltk - stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for word in sample_text.split():\n",
    "    print(word,ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " programm program with program languag\n"
     ]
    }
   ],
   "source": [
    "# second nltk method\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    " \n",
    "ps = PorterStemmer()\n",
    " \n",
    "sentence = \"Programmers program with programming languages\"\n",
    "words = word_tokenize(sentence)\n",
    " \n",
    "# using reduce to apply stemmer to each word and join them back into a string\n",
    "stemmed_sentence = reduce(lambda x, y: x + \" \" + ps.stem(y), words, \"\")\n",
    " \n",
    "print(stemmed_sentence)\n",
    "#This code is contrinuted by Pushpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "happi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "print(ps.stem(\"running\"))  # Output: run\n",
    "print(ps.stem(\"happiness\"))  # Output: happi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy - stemming\n",
    "\n",
    "# - It might be surprising to you but spaCy doesn't contain any function for stemming as it relies on lemmatization only. Therefore, in this section, we will use NLTK for stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regex exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=K8L6KVGG-7o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, Python uses backslashes as escape characters. Prefacing the string definition with 'r' is a useful way to define a string where you need the backslash to be an actual backslash and not part of an escape code that means something else in the string.22 Feb 2022\n",
    "- usually \\n means newline \\t is tab, but rawstring will make everything compiled as it is\n",
    "\n",
    "F-string is a way to format strings in Python. It was introduced in Python 3.6 and aims to make it easier for users to add variables, comma separators, do padding with zeros and date format. Python String. | Image: Frank Andrade. F-string was introduced in Python 3.6 and provides a better way to format strings.14 Mar 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['support@example.com', 'sales@example.com']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Contact us at support@example.com or sales@example.com\"\n",
    "text = \"\"\"\n",
    "RegExr was created by gskinner.com.\n",
    "\n",
    "Edit the Expression & Text to see matches. Roll over matches or the expression for details. PCRE & JavaScript flavors of RegEx are supported. Validate your expression with Tests mode.\n",
    "\n",
    "The side bar includes a Cheatsheet, full Reference, and Help. You can also Save & Share with the Community and view patterns you create or favorite in My Patterns.\n",
    "\n",
    "Explore results with the Tools below. Replace & List output custom results. Details lists capture groups. Explain describes your expression in plain English.\n",
    "\"\"\"\n",
    "emails = re.findall(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", text1)\n",
    "print(emails)  # Output: ['support@example.com', 'sales@example.com']\n",
    "\n",
    "regex_test = re.findall(r\"/the/\",text)\n",
    "print(regex_test)\n",
    "\n",
    "# alternative is to do things like looks for @ and .com but others might be included like home address or websites, so regex is a more powerful way to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = '''\n",
    "abcdefghijklmnopqurtuvwxyz\n",
    "ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "1234567890\n",
    "\n",
    "Ha HaHa\n",
    "\n",
    "MetaCharacters (Need to be escaped):\n",
    ". ^ $ * + ? { } [ ] \\ | ( )\n",
    "\n",
    "coreyms.com\n",
    "\n",
    "321-555-4321\n",
    "123.555.1234\n",
    "123*555*1234\n",
    "800-555-1234\n",
    "900-555-1234\n",
    "\n",
    "Mr. Schafer\n",
    "Mr Smith\n",
    "Ms Davis\n",
    "Mrs. Robinson\n",
    "Mr. T\n",
    "mr rilwan\n",
    "mR. nabil\n",
    "MS. amanda\n",
    "Ms. TAn\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 4), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'abc')\n",
    "matches = pattern.finditer(text2)\n",
    "for m in matches:\n",
    "    print(m) # span tells you which index of the string gives that match, this is why finditer is useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! use regex to find patterns, exact match can just use regualr python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pattern.findall just returns all the actual string matches in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'cba')\n",
    "matches = pattern.finditer(text2)\n",
    "for m in matches:\n",
    "    print(m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(67, 69), match='Ha'>\n",
      "<re.Match object; span=(70, 72), match='Ha'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'\\bHa')\n",
    "matches = pattern.finditer(text2)\n",
    "for m in matches:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(72, 74), match='Ha'>\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'\\BHa')\n",
    "matches = pattern.finditer(text2)\n",
    "for m in matches:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321-555-432\n",
      "123.555.123\n",
      "123*555*123\n",
      "800-555-123\n",
      "900-555-123\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'\\d\\d\\d\\D\\d\\d\\d\\D\\d\\d\\d')\n",
    "matches = pattern.findall(text2)\n",
    "for m in matches:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or use . -> . in regex matches any char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321-555-432\n",
      "123.555.123\n",
      "123*555*123\n",
      "800-555-123\n",
      "900-555-123\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'\\d\\d\\d.\\d\\d\\d.\\d\\d\\d')\n",
    "matches = pattern.findall(text2)\n",
    "for m in matches:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if only want to match those with - in number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321-555-432\n",
      "800-555-123\n",
      "900-555-123\n",
      "\n",
      "123*555*123\n",
      "\n",
      "321-555-432\n",
      "123.555.123\n",
      "123*555*123\n",
      "800-555-123\n",
      "900-555-123\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'\\d\\d\\d[-]\\d\\d\\d[-]\\d\\d\\d')\n",
    "matches = pattern.findall(text2)\n",
    "for m in matches:\n",
    "    print(m)\n",
    "print()\n",
    "\n",
    "pattern = re.compile(r'\\d\\d\\d[*]\\d\\d\\d[*]\\d\\d\\d')\n",
    "matches = pattern.findall(text2)\n",
    "for m in matches:\n",
    "    print(m)\n",
    "print()\n",
    "\n",
    "pattern = re.compile(r'\\d\\d\\d[-*.]\\d\\d\\d[-*.]\\d\\d\\d')\n",
    "# matches any char in []\n",
    "matches = pattern.findall(text2)\n",
    "for m in matches:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that in pattern = re.compile(r'\\d\\d\\d[-*.]\\d\\d\\d[-*.]\\d\\d\\d')\n",
    "the [-*.] still only matches one char, even if you have\n",
    "[A-Za-z0-9.] it still only matches one char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800-555-123\n",
      "900-555-123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'[89]00[-]\\d\\d\\d[-]\\d\\d\\d')\n",
    "matches = pattern.findall(text2)\n",
    "for m in matches:\n",
    "    print(m)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "u\n",
      "r\n",
      "t\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n",
      "a\n",
      "a\n",
      "a\n",
      "e\n",
      "t\n",
      "a\n",
      "h\n",
      "a\n",
      "r\n",
      "a\n",
      "c\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      "e\n",
      "e\n",
      "d\n",
      "t\n",
      "o\n",
      "b\n",
      "e\n",
      "e\n",
      "s\n",
      "c\n",
      "a\n",
      "p\n",
      "e\n",
      "d\n",
      "c\n",
      "o\n",
      "r\n",
      "e\n",
      "y\n",
      "m\n",
      "s\n",
      "c\n",
      "o\n",
      "m\n",
      "r\n",
      "c\n",
      "h\n",
      "a\n",
      "f\n",
      "e\n",
      "r\n",
      "r\n",
      "m\n",
      "i\n",
      "t\n",
      "h\n",
      "s\n",
      "a\n",
      "v\n",
      "i\n",
      "s\n",
      "r\n",
      "s\n",
      "o\n",
      "b\n",
      "i\n",
      "n\n",
      "s\n",
      "o\n",
      "n\n",
      "r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'[a-z]')\n",
    "matches = pattern.findall(text2)\n",
    "for m in matches:\n",
    "    print(m)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n",
      "K\n",
      "L\n",
      "M\n",
      "N\n",
      "O\n",
      "P\n",
      "Q\n",
      "R\n",
      "S\n",
      "T\n",
      "U\n",
      "V\n",
      "W\n",
      "X\n",
      "Y\n",
      "Z\n",
      "H\n",
      "H\n",
      "H\n",
      "M\n",
      "C\n",
      "N\n",
      "M\n",
      "S\n",
      "M\n",
      "S\n",
      "M\n",
      "D\n",
      "M\n",
      "R\n",
      "M\n",
      "T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'[A-Z]')\n",
    "matches = pattern.findall(text2)\n",
    "for m in matches:\n",
    "    print(m)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "2\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'[1-5]')\n",
    "matches = pattern.findall(text2)\n",
    "for m in matches:\n",
    "    print(m)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "u\n",
      "r\n",
      "t\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n",
      "K\n",
      "L\n",
      "M\n",
      "N\n",
      "O\n",
      "P\n",
      "Q\n",
      "R\n",
      "S\n",
      "T\n",
      "U\n",
      "V\n",
      "W\n",
      "X\n",
      "Y\n",
      "Z\n",
      "H\n",
      "a\n",
      "H\n",
      "a\n",
      "H\n",
      "a\n",
      "M\n",
      "e\n",
      "t\n",
      "a\n",
      "C\n",
      "h\n",
      "a\n",
      "r\n",
      "a\n",
      "c\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      "N\n",
      "e\n",
      "e\n",
      "d\n",
      "t\n",
      "o\n",
      "b\n",
      "e\n",
      "e\n",
      "s\n",
      "c\n",
      "a\n",
      "p\n",
      "e\n",
      "d\n",
      "c\n",
      "o\n",
      "r\n",
      "e\n",
      "y\n",
      "m\n",
      "s\n",
      "c\n",
      "o\n",
      "m\n",
      "M\n",
      "r\n",
      "S\n",
      "c\n",
      "h\n",
      "a\n",
      "f\n",
      "e\n",
      "r\n",
      "M\n",
      "r\n",
      "S\n",
      "m\n",
      "i\n",
      "t\n",
      "h\n",
      "M\n",
      "s\n",
      "D\n",
      "a\n",
      "v\n",
      "i\n",
      "s\n",
      "M\n",
      "r\n",
      "s\n",
      "R\n",
      "o\n",
      "b\n",
      "i\n",
      "n\n",
      "s\n",
      "o\n",
      "n\n",
      "M\n",
      "r\n",
      "T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'[a-zA-Z]')\n",
    "matches = pattern.findall(text2)\n",
    "for m in matches:\n",
    "    print(m)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'mat', 'pat']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3 = \"cat mat pat bat dog\"\n",
    "# to get all with at the back but not starting with b, can do with [^ ]\n",
    "pattern = re.compile(r'[^b]at')\n",
    "matches = re.findall(pattern,text3)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['321-555-432', '123.555.123', '123*555*123', '800-555-123', '900-555-123']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way to do pattern = re.compile(r'\\d\\d\\d.\\d\\d\\d.\\d\\d\\d')\n",
    "\n",
    "pattern = re.compile(r'\\d{3}.\\d{3}.\\d{3}')\n",
    "matches = re.findall(pattern,text2)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "abcdefghijklmnopqurtuvwxyz\n",
      "ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      "1234567890\n",
      "\n",
      "Ha HaHa\n",
      "\n",
      "MetaCharacters (Need to be escaped):\n",
      ". ^ $ * + ? { } [ ] \\ | ( )\n",
      "\n",
      "coreyms.com\n",
      "\n",
      "321-555-4321\n",
      "123.555.1234\n",
      "123*555*1234\n",
      "800-555-1234\n",
      "900-555-1234\n",
      "\n",
      "Mr. Schafer\n",
      "Mr Smith\n",
      "Ms Davis\n",
      "Mrs. Robinson\n",
      "Mr. T\n",
      "mr rilwan\n",
      "mR. nabil\n",
      "MS. amanda\n",
      "Ms. TAn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Schafer',\n",
       " 'Mr Smith',\n",
       " 'Ms Davis',\n",
       " 'Mrs. Robinson',\n",
       " 'Mr. T',\n",
       " 'mr rilwan',\n",
       " 'mR. nabil',\n",
       " 'MS. amanda',\n",
       " 'Ms. TAn']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try and get all the names + designation even tho some have . after Mr some dont etc\n",
    "# example\n",
    "# Mr. Schafer\n",
    "# Mr Smith\n",
    "# Ms Davis\n",
    "# Mrs. Robinson\n",
    "# Mr. T\n",
    "\n",
    "pattern = re.compile(r'[mM][rRsS][sS]?[\\.]?\\s\\w*')\n",
    "matches = re.findall(pattern,text2)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ms', 'Mr', 'Mr', 'Ms', 'Mr', 'Mr', 'mr', 'Ms']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try and get all the names + designation even tho some have . after Mr some dont etc\n",
    "# example\n",
    "# Mr. Schafer\n",
    "# Mr Smith\n",
    "# Ms Davis\n",
    "# Mrs. Robinson\n",
    "# Mr. T\n",
    "\n",
    "pattern = re.compile(r'[mM][(r|s|rs)]')\n",
    "matches = re.findall(pattern,text2)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r', 'r', 's', 'r', 'r', 's']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try and get all the names + designation even tho some have . after Mr some dont etc\n",
    "# example\n",
    "# Mr. Schafer\n",
    "# Mr Smith\n",
    "# Ms Davis\n",
    "# Mrs. Robinson\n",
    "# Mr. T\n",
    "\n",
    "pattern = re.compile(r'M(r|s|rs)\\.?')\n",
    "matches = re.findall(pattern,text2)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is returning ['r', 'r', 's', 'r', 'r', 's'] is because of the capturing group (r|s|rs). This capturing group captures only the portion inside the parentheses (i.e., \"r\", \"s\", or \"rs\"), which explains why only those letters are being returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Schafer',\n",
       " 'Mr Smith',\n",
       " 'Ms Davis',\n",
       " 'Mr. T',\n",
       " 'mr rilwan',\n",
       " 'mR. nabil',\n",
       " 'MS. amanda',\n",
       " 'Ms. TAn']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try and get all the names + designation even tho some have . after Mr some dont etc\n",
    "# example\n",
    "# Mr. Schafer\n",
    "# Mr Smith\n",
    "# Ms Davis\n",
    "# Mrs. Robinson\n",
    "# Mr. T\n",
    "\n",
    "pattern = re.compile(r'[mM][(rR|sS|rs|rS|Rs)]\\.?\\s\\w*')\n",
    "matches = re.findall(pattern,text2)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CoreyMSchafer@gmail.c', 'schafer@university.e']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try catch the emails\n",
    "\n",
    "emails = '''\n",
    "CoreyMSchafer@gmail.com\n",
    "corey.schafer@university.edu\n",
    "corey-321-schafer@my-work.net\n",
    "'''\n",
    "\n",
    "pattern = re.compile(r'\\w*@\\w*\\.[com|edu|net]')\n",
    "matches = re.findall(pattern,emails)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[com|edu] checks for characters, c o m e d u only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['com', 'edu']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try catch the emails\n",
    "\n",
    "emails = '''\n",
    "CoreyMSchafer@gmail.com\n",
    "corey.schafer@university.edu\n",
    "corey-321-schafer@my-work.net\n",
    "'''\n",
    "\n",
    "# pattern = re.compile(r'\\w*@\\w*\\.(com|edu|net)')\n",
    "# pattern = re.compile(r'\\w+@\\w+\\.(com|edu|net)')\n",
    "# pattern = re.compile(r'\\w*@\\w*\\.(com)')\n",
    "pattern = re.compile(r'[a-zA-Z.]+@[a-zA-Z]+\\.(com|edu)')\n",
    " \n",
    "\n",
    "matches = re.findall(pattern,emails)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you cant use findall with a group as it will only return whats in the group, should use finditer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<re.Match object; span=(1, 24), match='CoreyMSchafer@gmail.com'>,\n",
       " <re.Match object; span=(25, 53), match='corey.schafer@university.edu'>,\n",
       " <re.Match object; span=(54, 83), match='corey-321-schafer@my-work.net'>,\n",
       " <re.Match object; span=(84, 113), match='corey-321+schafer@my-work.net'>]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try catch the emails\n",
    "\n",
    "emails = '''\n",
    "CoreyMSchafer@gmail.com\n",
    "corey.schafer@university.edu\n",
    "corey-321-schafer@my-work.net\n",
    "corey-321+schafer@my-work.net\n",
    "'''\n",
    "\n",
    "# pattern = re.compile(r'\\w*@\\w*\\.(com|edu|net)')\n",
    "# pattern = re.compile(r'\\w+@\\w+\\.(com|edu|net)')\n",
    "# pattern = re.compile(r'\\w*@\\w*\\.(com)')\n",
    "pattern = re.compile(r'[a-zA-Z0-9.+-]+@[a-zA-Z-]+\\.(com|edu|net)')\n",
    "\n",
    "matches = [match for match in re.finditer(pattern,emails)]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bcacaca', '', '', 'caca', '', '', 'bcaca', '']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example that works with findall\n",
    " \n",
    "text = 'bcacaca dcaca dbcaca'\n",
    "pattern = 'b?(?:.a)*'\n",
    "\n",
    "## Evaluate regex\n",
    "result = re.findall(pattern, text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<re.Match object; span=(1, 23), match='https://www.google.com'>,\n",
       " <re.Match object; span=(24, 42), match='http://coreyms.com'>,\n",
       " <re.Match object; span=(43, 62), match='https://youtube.com'>,\n",
       " <re.Match object; span=(63, 83), match='https://www.nasa.gov'>]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "urls = '''\n",
    "https://www.google.com\n",
    "http://coreyms.com\n",
    "https://youtube.com\n",
    "https://www.nasa.gov\n",
    "'''\n",
    "\n",
    "pattern = re.compile(r'https?://[a-zA-Z.]+')\n",
    "matches = [match for match in re.finditer(pattern,urls)]\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<re.Match object; span=(1, 23), match='https://www.google.com'>,\n",
       " <re.Match object; span=(24, 42), match='http://coreyms.com'>,\n",
       " <re.Match object; span=(43, 62), match='https://youtube.com'>,\n",
       " <re.Match object; span=(63, 83), match='https://www.nasa.gov'>]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "urls = '''\n",
    "https://www.google.com\n",
    "http://coreyms.com\n",
    "https://youtube.com\n",
    "https://www.nasa.gov\n",
    "'''\n",
    "\n",
    "pattern = re.compile(r'https?://(www\\.)?\\w+\\.\\w+')\n",
    "matches = [match for match in re.finditer(pattern,urls)]\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only using groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<re.Match object; span=(1, 23), match='https://www.google.com'>,\n",
       " <re.Match object; span=(24, 42), match='http://coreyms.com'>,\n",
       " <re.Match object; span=(43, 62), match='https://youtube.com'>,\n",
       " <re.Match object; span=(63, 83), match='https://www.nasa.gov'>]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "urls = '''\n",
    "https://www.google.com\n",
    "http://coreyms.com\n",
    "https://youtube.com\n",
    "https://www.nasa.gov\n",
    "'''\n",
    "\n",
    "pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)')\n",
    "matches = [match for match in re.finditer(pattern,urls)]\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by doing everything in groups you can find them in the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com\n",
      "www.\n",
      "google\n",
      ".com\n",
      "\n",
      "\n",
      "http://coreyms.com\n",
      "None\n",
      "coreyms\n",
      ".com\n",
      "\n",
      "\n",
      "https://youtube.com\n",
      "None\n",
      "youtube\n",
      ".com\n",
      "\n",
      "\n",
      "https://www.nasa.gov\n",
      "www.\n",
      "nasa\n",
      ".gov\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match in matches:\n",
    "    print(match.group(0)) #returns entire match\n",
    "    print(match.group(1)) #returns the matches for the first group meaning first ()\n",
    "    print(match.group(2))\n",
    "    print(match.group(3))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the groups, you directly call them in the sub method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "google.com\n",
      "coreyms.com\n",
      "youtube.com\n",
      "nasa.gov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example here to sub the url with only the group 2 and 3 to shorten it\n",
    "\n",
    "pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)')\n",
    "subbed_urls = pattern.sub(r'\\2\\3',urls)\n",
    "print(subbed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "google.....com\n",
      "coreyms.....com\n",
      "youtube.....com\n",
      "nasa.....gov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)')\n",
    "subbed_urls = pattern.sub(r'\\2....\\3',urls) #the ... works in r string because this is not being compiled by the regex\n",
    "print(subbed_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.findall just finds the match and returns a list of matches\n",
    "- in a group it only looks for the return in those groups, or in multiple groups it returns list of tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.match returns only the first match at the beginning of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='Start'>\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Start a sentence and the bring it to and end.'\n",
    "pattern = re.compile('Start')\n",
    "matches =  pattern.match(sentence)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(8, 16), match='sentence'>\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Start a sentence and the bring it to and end.'\n",
    "pattern = re.compile('sentence')\n",
    "matches =  pattern.search(sentence)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flags, re.IGNORECASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='Start'>\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Start a sentence and the bring it to and end.'\n",
    "pattern = re.compile('start',re.IGNORECASE)\n",
    "matches =  pattern.match(sentence)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing tutorial\n",
    "* follow this series: https://medium.com/@erhan_arslan/understanding-natural-language-processing-nlp-step-1-bd5030c5a1b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try and get all the names + designation even tho some have . after Mr some dont etc\n",
    "# example\n",
    "# Mr. Schafer\n",
    "# Mr Smith\n",
    "# Ms Davis\n",
    "# Mrs. Robinson\n",
    "# Mr. T\n",
    "\n",
    "pattern = re.compile(r'[mM][(r|s|rs)]')\n",
    "matches = re.findall(pattern,text2)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MT proj preprocessing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Product Allocation (PAL) in advanced Available-to-Promise (aATP) is a mechanism in SAP S/4HANA that helps avoid critical situations in demand and procurement. It allows the allocation of materials in short supply to specific regions and customers for a specific time period. This ensures that the entire available quantity of a material is not allocated to a single customer, enabling subsequent order requirements from other customers to be confirmed. PAL helps in precise planning and control of material delivery to meet customer demands.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "\n",
    "    tokens = word_tokenize(text, preserve_line=True)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [i for i in tokens if not i in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product allocation (pal) in advanced available-to-promise (aatp) is a mechanism in sap s/hana that helps avoid critical situations in demand and procurement. it allows the allocation of materials in short supply to specific regions and customers for a specific time period. this ensures that the entire available quantity of a material is not allocated to a single customer, enabling subsequent order requirements from other customers to be confirmed. pal helps in precise planning and control of material delivery to meet customer demands.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(sample_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
